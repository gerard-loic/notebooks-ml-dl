{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture Transformer pour le NLP : G√©n√©ration de Texte en Fran√ßais\n",
    "\n",
    "## Objectifs de ce notebook\n",
    "\n",
    "Dans ce notebook, nous allons explorer l'architecture r√©volutionnaire des **Transformers** pour le traitement du langage naturel (NLP) :\n",
    "\n",
    "1. **Les r√©seaux r√©currents (LSTM)** - L'approche traditionnelle\n",
    "2. **L'architecture Transformer** - La r√©volution de l'attention\n",
    "3. **Le m√©canisme d'attention** - Comprendre comment √ßa fonctionne\n",
    "4. **La g√©n√©ration de texte** - Application pratique\n",
    "\n",
    "Nous utiliserons un corpus de **proverbes et citations en fran√ßais** pour entra√Æner nos mod√®les √† g√©n√©rer du texte.\n",
    "\n",
    "## Pourquoi les Transformers ?\n",
    "\n",
    "Les Transformers ont r√©volutionn√© le NLP en 2017 (papier \"Attention is All You Need\") :\n",
    "- Parall√©lisation massive de l'entra√Ænement (vs RNN s√©quentiels)\n",
    "- Capture de d√©pendances √† longue distance\n",
    "- Base de GPT, BERT, T5, et tous les LLMs modernes\n",
    "- Architecture unifi√©e pour de nombreuses t√¢ches NLP\n",
    "\n",
    "## üìö Concepts cl√©s que nous allons explorer\n",
    "\n",
    "- **Self-Attention** : Comment les mots \"se regardent\" entre eux\n",
    "- **Multi-Head Attention** : Plusieurs perspectives en parall√®le\n",
    "- **Positional Encoding** : Encoder la position des mots\n",
    "- **Feed-Forward Networks** : Transformation non-lin√©aire\n",
    "- **Layer Normalization** : Stabilisation de l'entra√Ænement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioth√®ques principales\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output, HTML\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Configuration GPU\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úÖ GPU(s) d√©tect√©(s): {len(gpus)} - Croissance m√©moire activ√©e\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Aucun GPU d√©tect√© - Utilisation du CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"Configuration GPU: {e}\")\n",
    "\n",
    "# Configuration graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproductibilit√©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"\\nüì¶ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üì¶ Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cr√©ation du Dataset : Proverbes et Citations Fran√ßais\n",
    "\n",
    "Pour ce tutoriel, nous allons cr√©er un corpus de proverbes, citations et phrases en fran√ßais.\n",
    "C'est un dataset simple mais suffisant pour comprendre les m√©canismes des Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus √©tendu de textes en fran√ßais (400+ phrases)\n",
    "corpus_francais = [\n",
    "    # Proverbes fran√ßais classiques (50 proverbes)\n",
    "    \"Petit √† petit, l'oiseau fait son nid.\",\n",
    "    \"Qui vivra verra.\",\n",
    "    \"L'habit ne fait pas le moine.\",\n",
    "    \"Pierre qui roule n'amasse pas mousse.\",\n",
    "    \"Tout vient √† point √† qui sait attendre.\",\n",
    "    \"La nuit porte conseil.\",\n",
    "    \"Mieux vaut tard que jamais.\",\n",
    "    \"Il n'y a pas de fum√©e sans feu.\",\n",
    "    \"Les chiens aboient, la caravane passe.\",\n",
    "    \"Chat √©chaud√© craint l'eau froide.\",\n",
    "    \"Qui ne tente rien n'a rien.\",\n",
    "    \"L'argent ne fait pas le bonheur.\",\n",
    "    \"Les jours se suivent et ne se ressemblent pas.\",\n",
    "    \"Il faut battre le fer tant qu'il est chaud.\",\n",
    "    \"Qui s√®me le vent r√©colte la temp√™te.\",\n",
    "    \"Deux pr√©cautions valent mieux qu'une.\",\n",
    "    \"Ventre affam√© n'a point d'oreilles.\",\n",
    "    \"Qui ne risque rien n'a rien.\",\n",
    "    \"Apr√®s la pluie, le beau temps.\",\n",
    "    \"L'union fait la force.\",\n",
    "    \"Les grands esprits se rencontrent.\",\n",
    "    \"Loin des yeux, loin du c≈ìur.\",\n",
    "    \"Qui dort d√Æne.\",\n",
    "    \"La parole est d'argent, le silence est d'or.\",\n",
    "    \"Il n'est jamais trop tard pour bien faire.\",\n",
    "    \"Rira bien qui rira le dernier.\",\n",
    "    \"Tous les chemins m√®nent √† Rome.\",\n",
    "    \"Une hirondelle ne fait pas le printemps.\",\n",
    "    \"Qui aime bien ch√¢tie bien.\",\n",
    "    \"La fin justifie les moyens.\",\n",
    "    \"Tel p√®re, tel fils.\",\n",
    "    \"Comme on fait son lit, on se couche.\",\n",
    "    \"Un tiens vaut mieux que deux tu l'auras.\",\n",
    "    \"Aux innocents les mains pleines.\",\n",
    "    \"Quand le chat n'est pas l√†, les souris dansent.\",\n",
    "    \"Il faut tourner sept fois sa langue dans sa bouche.\",\n",
    "    \"Qui vole un ≈ìuf vole un b≈ìuf.\",\n",
    "    \"√Ä chaque jour suffit sa peine.\",\n",
    "    \"Les petits ruisseaux font les grandes rivi√®res.\",\n",
    "    \"Paris ne s'est pas fait en un jour.\",\n",
    "    \"Tant va la cruche √† l'eau qu'√† la fin elle se casse.\",\n",
    "    \"La faim chasse le loup du bois.\",\n",
    "    \"L'homme propose et Dieu dispose.\",\n",
    "    \"Chose promise, chose due.\",\n",
    "    \"Les absents ont toujours tort.\",\n",
    "    \"La raison du plus fort est toujours la meilleure.\",\n",
    "    \"√Ä bon chat, bon rat.\",\n",
    "    \"Les conseilleurs ne sont pas les payeurs.\",\n",
    "    \"Comparaison n'est pas raison.\",\n",
    "    \"Il ne faut pas vendre la peau de l'ours avant de l'avoir tu√©.\",\n",
    "    \n",
    "    # Citations sur la vie (60 citations)\n",
    "    \"La vie est belle quand on sait la regarder.\",\n",
    "    \"Le savoir est la seule richesse qu'on ne peut pas voler.\",\n",
    "    \"Un sourire co√ªte moins cher que l'√©lectricit√© mais donne autant de lumi√®re.\",\n",
    "    \"Le bonheur n'est pas une destination, c'est une fa√ßon de voyager.\",\n",
    "    \"Chaque jour est une nouvelle chance de changer sa vie.\",\n",
    "    \"La patience est la cl√© de la r√©ussite.\",\n",
    "    \"Les r√™ves sont les graines de la r√©alit√©.\",\n",
    "    \"L'√©chec est le fondement de la r√©ussite.\",\n",
    "    \"Le temps gu√©rit toutes les blessures.\",\n",
    "    \"La v√©rit√© finit toujours par triompher.\",\n",
    "    \"L'amour est plus fort que la haine.\",\n",
    "    \"La connaissance √©claire l'esprit.\",\n",
    "    \"Le travail acharn√© porte ses fruits.\",\n",
    "    \"La curiosit√© est le moteur de la d√©couverte.\",\n",
    "    \"L'honn√™tet√© est toujours r√©compens√©e.\",\n",
    "    \"La simplicit√© est la sophistication supr√™me.\",\n",
    "    \"Le silence est parfois la meilleure r√©ponse.\",\n",
    "    \"L'espoir fait vivre.\",\n",
    "    \"La beaut√© est dans l'≈ìil de celui qui regarde.\",\n",
    "    \"Le voyage est plus important que la destination.\",\n",
    "    \"La libert√© n'a pas de prix.\",\n",
    "    \"L'imagination est plus importante que le savoir.\",\n",
    "    \"La qualit√© vaut mieux que la quantit√©.\",\n",
    "    \"Le pr√©sent est un cadeau pr√©cieux.\",\n",
    "    \"La musique adoucit les m≈ìurs.\",\n",
    "    \"L'√©ducation ouvre toutes les portes.\",\n",
    "    \"La nature est un livre ouvert.\",\n",
    "    \"Le rire est le meilleur des rem√®des.\",\n",
    "    \"La gratitude transforme ce que nous avons en suffisance.\",\n",
    "    \"Le changement est la seule constante.\",\n",
    "    \"La vie commence l√† o√π commence ta zone de confort.\",\n",
    "    \"Chaque instant est un nouveau d√©part.\",\n",
    "    \"La pers√©v√©rance est le chemin du succ√®s.\",\n",
    "    \"Les obstacles sont des opportunit√©s d√©guis√©es.\",\n",
    "    \"Le courage, c'est d'avoir peur et d'avancer quand m√™me.\",\n",
    "    \"La sagesse vient avec l'exp√©rience.\",\n",
    "    \"Le meilleur moment pour planter un arbre √©tait il y a vingt ans.\",\n",
    "    \"Le second meilleur moment est maintenant.\",\n",
    "    \"La confiance en soi est le premier secret du succ√®s.\",\n",
    "    \"Les grandes choses ont de petits commencements.\",\n",
    "    \"L'optimisme est une forme de courage.\",\n",
    "    \"La discipline est le pont entre les objectifs et l'accomplissement.\",\n",
    "    \"Le succ√®s est la somme de petits efforts r√©p√©t√©s jour apr√®s jour.\",\n",
    "    \"La vie est trop courte pour √™tre petite.\",\n",
    "    \"Fais de ta vie un r√™ve et d'un r√™ve une r√©alit√©.\",\n",
    "    \"Le bonheur est une direction, pas un lieu.\",\n",
    "    \"Chaque expert a √©t√© un jour un d√©butant.\",\n",
    "    \"La motivation te fait commencer, l'habitude te fait continuer.\",\n",
    "    \"Le seul moyen de faire du bon travail est d'aimer ce que tu fais.\",\n",
    "    \"La cr√©ativit√© exige le courage de l√¢cher prise sur les certitudes.\",\n",
    "    \"L'action est la cl√© fondamentale de tout succ√®s.\",\n",
    "    \"Le pessimiste voit la difficult√© dans chaque opportunit√©.\",\n",
    "    \"L'optimiste voit l'opportunit√© dans chaque difficult√©.\",\n",
    "    \"La vie est comme une bicyclette, il faut avancer pour ne pas perdre l'√©quilibre.\",\n",
    "    \"Sois le changement que tu veux voir dans le monde.\",\n",
    "    \"Le talent gagne des matchs, mais le travail d'√©quipe gagne des championnats.\",\n",
    "    \"La vraie g√©n√©rosit√© envers l'avenir consiste √† tout donner au pr√©sent.\",\n",
    "    \"L'important n'est pas de convaincre mais de donner √† r√©fl√©chir.\",\n",
    "    \"Le doute est le commencement de la sagesse.\",\n",
    "    \"La perfection est atteinte non pas lorsqu'il n'y a plus rien √† ajouter.\",\n",
    "    \n",
    "    # Phrases sur la nature (50 phrases)\n",
    "    \"Le soleil brille dans le ciel bleu.\",\n",
    "    \"Les oiseaux chantent dans les arbres.\",\n",
    "    \"La pluie tombe doucement sur la terre.\",\n",
    "    \"Le vent souffle √† travers les feuilles.\",\n",
    "    \"Les fleurs poussent au printemps.\",\n",
    "    \"La lune √©claire la nuit √©toil√©e.\",\n",
    "    \"Les vagues d√©ferlent sur le rivage.\",\n",
    "    \"La neige recouvre les montagnes en hiver.\",\n",
    "    \"Les papillons volent de fleur en fleur.\",\n",
    "    \"L'eau de la rivi√®re coule paisiblement.\",\n",
    "    \"Les nuages dansent dans le ciel.\",\n",
    "    \"Le chant du rossignol r√©sonne dans la for√™t.\",\n",
    "    \"Les √©toiles scintillent dans la nuit noire.\",\n",
    "    \"Le parfum des roses embaume le jardin.\",\n",
    "    \"Les feuilles d'automne tombent doucement.\",\n",
    "    \"La ros√©e du matin perle sur l'herbe.\",\n",
    "    \"Le tonnerre gronde au loin.\",\n",
    "    \"L'arc-en-ciel appara√Æt apr√®s l'orage.\",\n",
    "    \"Les abeilles butinent les fleurs sauvages.\",\n",
    "    \"Le cr√©puscule peint le ciel de mille couleurs.\",\n",
    "    \"La mer s'√©tend √† perte de vue.\",\n",
    "    \"Les arbres se balancent sous la brise l√©g√®re.\",\n",
    "    \"Le givre recouvre les branches en hiver.\",\n",
    "    \"Les grenouilles coassent au bord de l'√©tang.\",\n",
    "    \"Le soleil se couche derri√®re les collines.\",\n",
    "    \"Les montagnes se dressent majestueusement.\",\n",
    "    \"La cascade tombe avec fracas.\",\n",
    "    \"Les champignons poussent dans le sous-bois.\",\n",
    "    \"Le brouillard enveloppe la vall√©e.\",\n",
    "    \"Les √©cureuils grimpent aux arbres.\",\n",
    "    \"La temp√™te fait rage sur l'oc√©an.\",\n",
    "    \"Les lucioles illuminent la nuit d'√©t√©.\",\n",
    "    \"Le coucou annonce le retour du printemps.\",\n",
    "    \"Les iris fleurissent au bord de l'eau.\",\n",
    "    \"La pleine lune √©claire le paysage.\",\n",
    "    \"Les hirondelles volent bas avant la pluie.\",\n",
    "    \"Le vent d'automne emporte les feuilles mortes.\",\n",
    "    \"Les bourgeons √©closent au printemps.\",\n",
    "    \"La brume matinale se dissipe lentement.\",\n",
    "    \"Les cigales chantent par les chaudes journ√©es d'√©t√©.\",\n",
    "    \"Le gel dessine des motifs sur les vitres.\",\n",
    "    \"Les nuages s'amoncellent avant l'orage.\",\n",
    "    \"Le vent du nord apporte le froid.\",\n",
    "    \"Les premi√®res gouttes de pluie rafra√Æchissent l'air.\",\n",
    "    \"La nature s'√©veille avec le printemps.\",\n",
    "    \"Les stalactites pendent du toit en hiver.\",\n",
    "    \"Le soleil r√©chauffe la terre.\",\n",
    "    \"Les champs de bl√© ondulent sous le vent.\",\n",
    "    \"La for√™t se pare de ses couleurs d'automne.\",\n",
    "    \"Les marmottes sifflent dans la montagne.\",\n",
    "    \n",
    "    # Phrases sur les actions et valeurs (60 phrases)\n",
    "    \"Il faut toujours croire en ses r√™ves.\",\n",
    "    \"Apprendre est un voyage sans fin.\",\n",
    "    \"Le courage n'est pas l'absence de peur.\",\n",
    "    \"Chaque erreur est une le√ßon pr√©cieuse.\",\n",
    "    \"La pers√©v√©rance m√®ne au succ√®s.\",\n",
    "    \"L'amiti√© est un tr√©sor inestimable.\",\n",
    "    \"Le respect est la base de toute relation.\",\n",
    "    \"La g√©n√©rosit√© enrichit celui qui donne.\",\n",
    "    \"La compassion ouvre les c≈ìurs.\",\n",
    "    \"L'empathie cr√©e des ponts entre les gens.\",\n",
    "    \"La tol√©rance est une vertu essentielle.\",\n",
    "    \"Le pardon lib√®re l'√¢me.\",\n",
    "    \"La bienveillance illumine le monde.\",\n",
    "    \"L'humilit√© est la marque des grands.\",\n",
    "    \"La sinc√©rit√© forge la confiance.\",\n",
    "    \"L'int√©grit√© guide nos actions.\",\n",
    "    \"La fid√©lit√© renforce les liens.\",\n",
    "    \"La loyaut√© est rare et pr√©cieuse.\",\n",
    "    \"La solidarit√© unit les communaut√©s.\",\n",
    "    \"L'entraide fait avancer l'humanit√©.\",\n",
    "    \"La coop√©ration multiplie les forces.\",\n",
    "    \"Le dialogue r√©sout les conflits.\",\n",
    "    \"L'√©coute est un art subtil.\",\n",
    "    \"La communication rapproche les individus.\",\n",
    "    \"La transparence √©tablit la confiance.\",\n",
    "    \"L'authenticit√© attire les autres.\",\n",
    "    \"La modestie sied aux sages.\",\n",
    "    \"La prudence √©vite bien des malheurs.\",\n",
    "    \"La sagesse vient avec l'√¢ge.\",\n",
    "    \"La r√©flexion pr√©c√®de l'action.\",\n",
    "    \"La m√©ditation apaise l'esprit.\",\n",
    "    \"La concentration am√©liore les performances.\",\n",
    "    \"La d√©termination surmonte les obstacles.\",\n",
    "    \"L'ambition pousse √† se d√©passer.\",\n",
    "    \"La volont√© forge le caract√®re.\",\n",
    "    \"La t√©nacit√© vient √† bout de tout.\",\n",
    "    \"La constance m√®ne √† l'excellence.\",\n",
    "    \"La r√©gularit√© engendre le progr√®s.\",\n",
    "    \"L'assiduit√© porte ses fruits.\",\n",
    "    \"Le d√©vouement m√©rite le respect.\",\n",
    "    \"L'engagement inspire les autres.\",\n",
    "    \"La passion anime l'existence.\",\n",
    "    \"L'enthousiasme est contagieux.\",\n",
    "    \"La joie de vivre illumine chaque instant.\",\n",
    "    \"L'optimisme transforme les d√©fis en opportunit√©s.\",\n",
    "    \"La gratitude multiplie le bonheur.\",\n",
    "    \"La reconnaissance honore les bienfaiteurs.\",\n",
    "    \"L'appr√©ciation valorise les petites choses.\",\n",
    "    \"La mindfulness ancre dans le pr√©sent.\",\n",
    "    \"La s√©r√©nit√© apporte la paix int√©rieure.\",\n",
    "    \"Le calme dans la temp√™te montre la force.\",\n",
    "    \"La r√©silience permet de rebondir.\",\n",
    "    \"L'adaptabilit√© est essentielle au changement.\",\n",
    "    \"La flexibilit√© ouvre de nouvelles voies.\",\n",
    "    \"L'innovation r√©volutionne le monde.\",\n",
    "    \"La cr√©ativit√© n'a pas de limites.\",\n",
    "    \"L'originalit√© distingue les artistes.\",\n",
    "    \"L'audace ouvre des portes.\",\n",
    "    \"Le courage inspire le respect.\",\n",
    "    \"La bravoure face au danger est admirable.\",\n",
    "    \n",
    "    # Phrases sur le temps et les saisons (40 phrases)\n",
    "    \"L'hiver apporte son manteau blanc.\",\n",
    "    \"Le printemps r√©veille la nature endormie.\",\n",
    "    \"L'√©t√© offre ses longues journ√©es ensoleill√©es.\",\n",
    "    \"L'automne peint les for√™ts de couleurs chaudes.\",\n",
    "    \"Les jours rallongent avec le retour du soleil.\",\n",
    "    \"Les nuits se font plus courtes en √©t√©.\",\n",
    "    \"Le temps passe et ne revient jamais.\",\n",
    "    \"Chaque saison a sa beaut√© propre.\",\n",
    "    \"Les ann√©es filent comme le vent.\",\n",
    "    \"Le temps est le plus pr√©cieux des biens.\",\n",
    "    \"L'instant pr√©sent est tout ce que nous avons.\",\n",
    "    \"Le pass√© nous enseigne, le futur nous inspire.\",\n",
    "    \"Aujourd'hui est le premier jour du reste de ta vie.\",\n",
    "    \"Demain est un autre jour.\",\n",
    "    \"Hier est derri√®re nous, demain est un myst√®re.\",\n",
    "    \"Le temps file entre nos doigts.\",\n",
    "    \"Les secondes s'√©gr√®nent inexorablement.\",\n",
    "    \"Les minutes deviennent des heures.\",\n",
    "    \"Les heures se transforment en jours.\",\n",
    "    \"Les jours composent les semaines.\",\n",
    "    \"Les semaines forment les mois.\",\n",
    "    \"Les mois constituent les ann√©es.\",\n",
    "    \"Le temps ne s'arr√™te jamais.\",\n",
    "    \"L'horloge tourne sans rel√¢che.\",\n",
    "    \"Le sablier s'√©coule grain par grain.\",\n",
    "    \"La vie est une course contre le temps.\",\n",
    "    \"Chaque moment compte dans notre existence.\",\n",
    "    \"Le temps perdu ne se rattrape jamais.\",\n",
    "    \"Il faut savoir profiter de l'instant.\",\n",
    "    \"Le moment pr√©sent est un cadeau.\",\n",
    "    \"L'√©ternit√© commence maintenant.\",\n",
    "    \"Le temps r√©v√®le toutes les v√©rit√©s.\",\n",
    "    \"La patience vient avec le temps.\",\n",
    "    \"Les blessures gu√©rissent avec le temps.\",\n",
    "    \"Le temps arrange beaucoup de choses.\",\n",
    "    \"Avec le temps va, tout s'en va.\",\n",
    "    \"Le temps est un grand ma√Ætre.\",\n",
    "    \"La sagesse s'acquiert avec le temps.\",\n",
    "    \"Le temps respecte ce qui est fait avec lui.\",\n",
    "    \"Chaque √¢ge a ses plaisirs et ses peines.\",\n",
    "    \n",
    "    # Phrases sur l'apprentissage et la connaissance (50 phrases)\n",
    "    \"Apprendre, c'est d√©couvrir ce que l'on sait d√©j√†.\",\n",
    "    \"La connaissance est un tr√©sor qui suit son propri√©taire partout.\",\n",
    "    \"Celui qui pose une question reste ignorant cinq minutes.\",\n",
    "    \"Celui qui ne la pose pas reste ignorant toute sa vie.\",\n",
    "    \"L'√©ducation est l'arme la plus puissante pour changer le monde.\",\n",
    "    \"Un livre est un r√™ve que l'on tient entre ses mains.\",\n",
    "    \"La lecture est √† l'esprit ce que l'exercice est au corps.\",\n",
    "    \"Les livres sont les amis les plus silencieux et les plus constants.\",\n",
    "    \"√âtudier sans r√©fl√©chir est vain, r√©fl√©chir sans √©tudier est dangereux.\",\n",
    "    \"L'ignorance est la nuit de l'esprit.\",\n",
    "    \"La curiosit√© est le moteur de l'intelligence.\",\n",
    "    \"Poser des questions est le premier pas vers la sagesse.\",\n",
    "    \"L'apprentissage est un tr√©sor qui suivra partout.\",\n",
    "    \"Plus on apprend, plus on r√©alise qu'on ne sait rien.\",\n",
    "    \"La vraie connaissance commence par la reconnaissance de son ignorance.\",\n",
    "    \"L'exp√©rience est le meilleur des enseignants.\",\n",
    "    \"On apprend de ses erreurs plus que de ses succ√®s.\",\n",
    "    \"Chaque √©chec est une opportunit√© d'apprendre.\",\n",
    "    \"La pratique rend parfait.\",\n",
    "    \"La r√©p√©tition est la m√®re de l'apprentissage.\",\n",
    "    \"Un bon professeur inspire pour l'√©ternit√©.\",\n",
    "    \"Enseigner, c'est apprendre deux fois.\",\n",
    "    \"L'√©cole de la vie n'a pas de vacances.\",\n",
    "    \"On n'a jamais fini d'apprendre.\",\n",
    "    \"La culture est ce qui reste quand on a tout oubli√©.\",\n",
    "    \"L'intelligence n'est pas de savoir beaucoup mais de bien utiliser ce qu'on sait.\",\n",
    "    \"La m√©moire est le gardien du savoir.\",\n",
    "    \"Comprendre, c'est commencer √† √™tre libre.\",\n",
    "    \"Le savoir-faire vaut mieux que le savoir.\",\n",
    "    \"La th√©orie sans la pratique est inutile.\",\n",
    "    \"La pratique sans la th√©orie est aveugle.\",\n",
    "    \"L'analyse critique aiguise l'esprit.\",\n",
    "    \"Le doute est le commencement de la science.\",\n",
    "    \"La science avance par essais et erreurs.\",\n",
    "    \"La recherche est une aventure passionnante.\",\n",
    "    \"La d√©couverte ne consiste pas √† chercher de nouveaux paysages.\",\n",
    "    \"Elle consiste √† avoir de nouveaux yeux.\",\n",
    "    \"L'innovation na√Æt de la combinaison de connaissances.\",\n",
    "    \"Les grands esprits discutent des id√©es.\",\n",
    "    \"Les esprits moyens discutent des √©v√©nements.\",\n",
    "    \"Les petits esprits discutent des personnes.\",\n",
    "    \"La philosophie enseigne √† penser par soi-m√™me.\",\n",
    "    \"La logique est la grammaire de la raison.\",\n",
    "    \"Les math√©matiques sont le langage de l'univers.\",\n",
    "    \"La science est organis√©e connaissance.\",\n",
    "    \"La sagesse est organis√©e vie.\",\n",
    "    \"Le g√©nie est fait de un pour cent d'inspiration.\",\n",
    "    \"Et de quatre-vingt-dix-neuf pour cent de transpiration.\",\n",
    "    \"L'intelligence artificielle commence o√π finit la n√¥tre.\",\n",
    "    \"La technologie devrait simplifier la vie, pas la compliquer.\",\n",
    "]\n",
    "\n",
    "print(f\"üìö Corpus cr√©√© : {len(corpus_francais)} phrases\")\n",
    "print(f\"\\nüìä R√©partition du corpus :\")\n",
    "print(f\"  ‚Ä¢ Proverbes fran√ßais classiques : ~50\")\n",
    "print(f\"  ‚Ä¢ Citations sur la vie : ~60\")\n",
    "print(f\"  ‚Ä¢ Phrases sur la nature : ~50\")\n",
    "print(f\"  ‚Ä¢ Actions et valeurs : ~60\")\n",
    "print(f\"  ‚Ä¢ Temps et saisons : ~40\")\n",
    "print(f\"  ‚Ä¢ Apprentissage et connaissance : ~50\")\n",
    "print(f\"\\nüìù Exemples de phrases :\")\n",
    "for i, phrase in enumerate(corpus_francais[:5], 1):\n",
    "    print(f\"  {i}. {phrase}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pr√©traitement du texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nettoyer_texte(texte):\n",
    "    \"\"\"Nettoie et normalise le texte.\"\"\"\n",
    "    # Convertir en minuscules\n",
    "    texte = texte.lower()\n",
    "    # Ajouter des espaces autour de la ponctuation\n",
    "    texte = re.sub(r\"([.,!?'])\", r\" \\1 \", texte)\n",
    "    # Supprimer les espaces multiples\n",
    "    texte = re.sub(r'\\s+', ' ', texte)\n",
    "    return texte.strip()\n",
    "\n",
    "# Nettoyer le corpus\n",
    "corpus_nettoye = [nettoyer_texte(phrase) for phrase in corpus_francais]\n",
    "\n",
    "print(\"Exemples de phrases nettoy√©es :\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal : {corpus_francais[i]}\")\n",
    "    print(f\"Nettoy√©  : {corpus_nettoye[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization\n",
    "\n",
    "La tokenization convertit le texte en s√©quences de nombres que le mod√®le peut comprendre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er le tokenizer\n",
    "tokenizer = Tokenizer(filters='', lower=False, oov_token='<UNK>')\n",
    "tokenizer.fit_on_texts(corpus_nettoye)\n",
    "\n",
    "# Statistiques du vocabulaire\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 pour le padding\n",
    "print(f\"üìñ Taille du vocabulaire : {vocab_size} mots\")\n",
    "print(f\"\\nüî§ Top 20 mots les plus fr√©quents :\")\n",
    "\n",
    "# Trier par fr√©quence\n",
    "word_freq = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "for i, (word, freq) in enumerate(word_freq, 1):\n",
    "    print(f\"  {i:2d}. '{word}' : {freq} fois\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Cr√©ation des s√©quences d'entra√Ænement\n",
    "\n",
    "Pour la g√©n√©ration de texte, nous cr√©ons des paires (contexte, mot suivant) :\n",
    "- Input : \"le chat mange\"\n",
    "- Output : \"la souris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_sequences(textes, tokenizer):\n",
    "    \"\"\"Cr√©e des s√©quences input-output pour l'entra√Ænement.\"\"\"\n",
    "    input_sequences = []\n",
    "    \n",
    "    for texte in textes:\n",
    "        # Convertir le texte en s√©quence de tokens\n",
    "        token_list = tokenizer.texts_to_sequences([texte])[0]\n",
    "        \n",
    "        # Cr√©er des n-grams progressifs\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    \n",
    "    return input_sequences\n",
    "\n",
    "# Cr√©er les s√©quences\n",
    "sequences = creer_sequences(corpus_nettoye, tokenizer)\n",
    "\n",
    "print(f\"\\nüìä Nombre total de s√©quences : {len(sequences)}\")\n",
    "print(f\"\\nüîç Exemples de s√©quences (premiers mots) :\")\n",
    "\n",
    "# Afficher quelques exemples\n",
    "exemple_phrase = corpus_nettoye[0]\n",
    "exemple_tokens = tokenizer.texts_to_sequences([exemple_phrase])[0]\n",
    "print(f\"\\nPhrase : '{exemple_phrase}'\")\n",
    "print(f\"Tokens : {exemple_tokens}\\n\")\n",
    "\n",
    "# Recr√©er les s√©quences pour cette phrase\n",
    "for i in range(1, min(6, len(exemple_tokens))):\n",
    "    seq = exemple_tokens[:i+1]\n",
    "    words = [list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(idx)] for idx in seq]\n",
    "    print(f\"  {seq[:-1]} ‚Üí {seq[-1]} | {' '.join(words[:-1])} ‚Üí {words[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding des s√©quences\n",
    "max_sequence_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='pre')\n",
    "\n",
    "# S√©paration X (input) et y (output)\n",
    "X = sequences[:, :-1]  # Tous les tokens sauf le dernier\n",
    "y = sequences[:, -1]   # Le dernier token\n",
    "\n",
    "# Convertir y en format cat√©goriel (one-hot encoding)\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=vocab_size)\n",
    "\n",
    "print(f\"\\nüìê Dimensions des donn√©es :\")\n",
    "print(f\"  X (input)  : {X.shape} - (nombre_sequences, longueur_sequence-1)\")\n",
    "print(f\"  y (output) : {y.shape} - (nombre_sequences, vocab_size)\")\n",
    "print(f\"\\n  Longueur maximale de s√©quence : {max_sequence_length}\")\n",
    "\n",
    "# Visualisation de la distribution des longueurs\n",
    "sequence_lengths = [len([x for x in seq if x != 0]) for seq in sequences]\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(sequence_lengths, bins=20, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Longueur de s√©quence', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "plt.title('Distribution des longueurs de s√©quences', fontsize=14, fontweight='bold')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mod√®le Baseline : LSTM (Approche Traditionnelle)\n",
    "\n",
    "Avant de plonger dans les Transformers, cr√©ons un mod√®le LSTM comme r√©f√©rence.\n",
    "\n",
    "### 3.1 Architecture LSTM\n",
    "\n",
    "Les LSTM (Long Short-Term Memory) sont des RNN am√©lior√©s qui :\n",
    "- Traitent les s√©quences de mani√®re **s√©quentielle** (un mot apr√®s l'autre)\n",
    "- Maintiennent une **m√©moire** des informations pass√©es\n",
    "- Peuvent capturer des **d√©pendances √† long terme**\n",
    "\n",
    "**Limitations** :\n",
    "- Traitement s√©quentiel (pas de parall√©lisation)\n",
    "- Difficult√© avec les tr√®s longues s√©quences\n",
    "- Gradient vanishing/exploding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_modele_lstm(vocab_size, max_length, embedding_dim=100, lstm_units=128):\n",
    "    \"\"\"Cr√©e un mod√®le LSTM pour la g√©n√©ration de texte.\"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Couche d'embedding : convertit les tokens en vecteurs denses\n",
    "        layers.Embedding(\n",
    "            input_dim=vocab_size,\n",
    "            output_dim=embedding_dim,\n",
    "            input_length=max_length - 1,\n",
    "            name='embedding'\n",
    "        ),\n",
    "        \n",
    "        # Couches LSTM\n",
    "        layers.LSTM(lstm_units, return_sequences=True, name='lstm_1'),\n",
    "        layers.Dropout(0.2, name='dropout_1'),\n",
    "        \n",
    "        layers.LSTM(lstm_units, name='lstm_2'),\n",
    "        layers.Dropout(0.2, name='dropout_2'),\n",
    "        \n",
    "        # Couche dense finale\n",
    "        layers.Dense(vocab_size, activation='softmax', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le LSTM\n",
    "lstm_model = creer_modele_lstm(vocab_size, max_sequence_length, embedding_dim=64, lstm_units=128)\n",
    "\n",
    "# Compiler\n",
    "lstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"üî® Mod√®le LSTM cr√©√© !\\n\")\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback pour visualiser la progression\n",
    "class TrainingProgressCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.accuracies.append(logs['accuracy'])\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "            \n",
    "            # Loss\n",
    "            ax1.plot(self.losses, color='red', linewidth=2, marker='o', markersize=4)\n",
    "            ax1.set_title('Loss (LSTM)', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch', fontsize=12)\n",
    "            ax1.set_ylabel('Loss', fontsize=12)\n",
    "            ax1.grid(alpha=0.3)\n",
    "            \n",
    "            # Accuracy\n",
    "            ax2.plot(self.accuracies, color='green', linewidth=2, marker='o', markersize=4)\n",
    "            ax2.set_title('Accuracy (LSTM)', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Epoch', fontsize=12)\n",
    "            ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "            ax2.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{self.params['epochs']} - \"\n",
    "                  f\"Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "\n",
    "# Callback\n",
    "progress_callback_lstm = TrainingProgressCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le LSTM\n",
    "print(\"üöÄ D√©but de l'entra√Ænement du mod√®le LSTM...\\n\")\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X, y,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    callbacks=[progress_callback_lstm]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement LSTM termin√© !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Test de g√©n√©ration de texte avec LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generer_texte(model, tokenizer, seed_text, num_words=10, temperature=1.0):\n",
    "    \"\"\"\n",
    "    G√©n√®re du texte √† partir d'un texte de d√©part.\n",
    "    \n",
    "    Parameters:\n",
    "    - temperature: Contr√¥le la cr√©ativit√© (0.5=conservateur, 1.0=√©quilibr√©, 2.0=cr√©atif)\n",
    "    \"\"\"\n",
    "    generated_text = seed_text\n",
    "    \n",
    "    for _ in range(num_words):\n",
    "        # Tokenize le texte actuel\n",
    "        token_list = tokenizer.texts_to_sequences([generated_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
    "        \n",
    "        # Pr√©diction\n",
    "        predictions = model.predict(token_list, verbose=0)[0]\n",
    "        \n",
    "        # Appliquer la temp√©rature\n",
    "        predictions = np.log(predictions + 1e-7) / temperature\n",
    "        predictions = np.exp(predictions) / np.sum(np.exp(predictions))\n",
    "        \n",
    "        # Choisir le prochain mot\n",
    "        predicted_id = np.random.choice(len(predictions), p=predictions)\n",
    "        \n",
    "        # Convertir l'ID en mot\n",
    "        predicted_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_id:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        \n",
    "        if predicted_word:\n",
    "            generated_text += \" \" + predicted_word\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# Tests de g√©n√©ration avec diff√©rents seeds\n",
    "print(\"üé≠ G√©n√©ration de texte avec LSTM\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "seeds = [\n",
    "    \"le soleil\",\n",
    "    \"la vie est\",\n",
    "    \"petit √† petit\",\n",
    "    \"l'amour\"\n",
    "]\n",
    "\n",
    "for seed in seeds:\n",
    "    generated = generer_texte(lstm_model, tokenizer, seed, num_words=8, temperature=0.8)\n",
    "    print(f\"\\nüå± Seed: '{seed}'\")\n",
    "    print(f\"üìù G√©n√©r√©: {generated}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. L'Architecture Transformer\n",
    "\n",
    "### 4.1 Les concepts fondamentaux\n",
    "\n",
    "Le Transformer repose sur plusieurs innovations cl√©s :\n",
    "\n",
    "#### 1. **Self-Attention (Auto-attention)**\n",
    "- Permet √† chaque mot de \"regarder\" tous les autres mots de la s√©quence\n",
    "- Calcule l'importance relative de chaque mot par rapport aux autres\n",
    "- Formule : Attention(Q, K, V) = softmax(QK^T / ‚àöd_k)V\n",
    "\n",
    "#### 2. **Multi-Head Attention**\n",
    "- Ex√©cute plusieurs m√©canismes d'attention en parall√®le\n",
    "- Chaque \"t√™te\" apprend √† se concentrer sur diff√©rents aspects\n",
    "- Exemple : une t√™te peut se concentrer sur la syntaxe, une autre sur la s√©mantique\n",
    "\n",
    "#### 3. **Positional Encoding**\n",
    "- Ajoute l'information de position aux embeddings\n",
    "- N√©cessaire car l'attention n'a pas de notion d'ordre intrins√®que\n",
    "- Utilise des fonctions sinuso√Ødales\n",
    "\n",
    "#### 4. **Feed-Forward Network**\n",
    "- R√©seau dense appliqu√© ind√©pendamment √† chaque position\n",
    "- Ajoute de la non-lin√©arit√© et de la capacit√© de mod√©lisation\n",
    "\n",
    "### 4.2 Impl√©mentation des composants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Cr√©e le positional encoding pour le Transformer.\n",
    "    Utilise des fonctions sinuso√Ødales pour encoder la position.\n",
    "    \"\"\"\n",
    "    positions = np.arange(seq_len)[:, np.newaxis]\n",
    "    dimensions = np.arange(d_model)[np.newaxis, :]\n",
    "    \n",
    "    angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float32(d_model))\n",
    "    angle_rads = positions * angle_rates\n",
    "    \n",
    "    # Appliquer sin aux indices pairs, cos aux indices impairs\n",
    "    pos_encoding = np.zeros((seq_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# Visualisation du positional encoding\n",
    "pos_encoding = get_positional_encoding(50, 128)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Dimension', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Position', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(label='Valeur')\n",
    "plt.title('Visualisation du Positional Encoding', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Le positional encoding ajoute une information unique √† chaque position.\")\n",
    "print(\"   Les patterns sinuso√Ødaux permettent au mod√®le de comprendre l'ordre des mots.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    \"\"\"\n",
    "    Impl√©mentation du Multi-Head Self-Attention.\n",
    "    \n",
    "    Cette couche est le c≈ìur du Transformer. Elle permet √† chaque position\n",
    "    de la s√©quence d'acc√©der √† toutes les autres positions.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert embed_dim % num_heads == 0, \"embed_dim doit √™tre divisible par num_heads\"\n",
    "        \n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Projections lin√©aires pour Q, K, V\n",
    "        self.query_dense = layers.Dense(embed_dim, name=\"query\")\n",
    "        self.key_dense = layers.Dense(embed_dim, name=\"key\")\n",
    "        self.value_dense = layers.Dense(embed_dim, name=\"value\")\n",
    "        \n",
    "        # Projection finale\n",
    "        self.combine_heads = layers.Dense(embed_dim, name=\"output\")\n",
    "        \n",
    "    def attention(self, query, key, value):\n",
    "        \"\"\"Calcule l'attention scaled dot-product avec masque de causalit√©.\"\"\"\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        \n",
    "        # Cr√©er le masque de causalit√© (look-ahead mask)\n",
    "        # Chaque position ne peut voir que les positions pr√©c√©dentes\n",
    "        seq_len = tf.shape(scaled_score)[-1]\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        mask = mask * -1e9  # Mettre -inf pour les positions futures\n",
    "        \n",
    "        scaled_score += mask\n",
    "        \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "    \n",
    "    def separate_heads(self, x, batch_size):\n",
    "        \"\"\"S√©pare la derni√®re dimension en (num_heads, projection_dim).\"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Projections lin√©aires\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "        \n",
    "        # S√©parer en multiple heads\n",
    "        query = self.separate_heads(query, batch_size)\n",
    "        key = self.separate_heads(key, batch_size)\n",
    "        value = self.separate_heads(value, batch_size)\n",
    "        \n",
    "        # Attention avec masque de causalit√©\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        \n",
    "        # Recombiner les heads\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        \n",
    "        # Projection finale\n",
    "        output = self.combine_heads(concat_attention)\n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ Classe MultiHeadSelfAttention d√©finie (avec masque de causalit√© int√©gr√©)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    Un bloc Transformer complet.\n",
    "    \n",
    "    Combine:\n",
    "    - Multi-Head Self-Attention\n",
    "    - Feed-Forward Network\n",
    "    - Layer Normalization\n",
    "    - Residual Connections\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-Head Attention\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # Feed-Forward Network\n",
    "        self.ffn = keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\"),\n",
    "            layers.Dense(embed_dim),\n",
    "        ])\n",
    "        \n",
    "        # Layer Normalization\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        # Multi-Head Attention avec residual connection\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        \n",
    "        # Feed-Forward Network avec residual connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        \n",
    "        return out2\n",
    "\n",
    "print(\"‚úÖ Classe TransformerBlock d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Combine l'embedding des tokens avec le positional encoding.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, max_len):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_encoding = get_positional_encoding(max_len, embed_dim)\n",
    "    \n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x = self.token_emb(x)\n",
    "        # Ajouter le positional encoding\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Classe PositionalEmbedding d√©finie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Construction du mod√®le Transformer complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creer_modele_transformer(vocab_size, max_length, embed_dim=128, num_heads=4, ff_dim=128, num_blocks=2):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le Transformer pour la g√©n√©ration de texte.\n",
    "    \n",
    "    Architecture:\n",
    "    Input ‚Üí Positional Embedding ‚Üí [Transformer Blocks] √ó N ‚Üí Derni√®re Position ‚Üí Dense ‚Üí Output\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = layers.Input(shape=(max_length - 1,), name=\"input_tokens\")\n",
    "    \n",
    "    # Positional Embedding\n",
    "    x = PositionalEmbedding(vocab_size, embed_dim, max_length)(inputs)\n",
    "    \n",
    "    # Empiler plusieurs blocs Transformer\n",
    "    # Le masque de causalit√© est maintenant int√©gr√© dans MultiHeadSelfAttention\n",
    "    for i in range(num_blocks):\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim, dropout_rate=0.1)(x)\n",
    "    \n",
    "    # IMPORTANT: Prendre uniquement la derni√®re position pour pr√©dire le prochain mot\n",
    "    # C'est crucial pour la g√©n√©ration de texte auto-r√©gressive\n",
    "    x = layers.Lambda(lambda x: x[:, -1, :])(x)  # Prend la derni√®re position\n",
    "    \n",
    "    # Couche de sortie\n",
    "    outputs = layers.Dense(vocab_size, activation=\"softmax\", name=\"output\")(x)\n",
    "    \n",
    "    # Cr√©er le mod√®le\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"transformer_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©er le mod√®le Transformer\n",
    "transformer_model = creer_modele_transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    max_length=max_sequence_length,\n",
    "    embed_dim=128,\n",
    "    num_heads=8,  # Augment√© de 4 √† 8\n",
    "    ff_dim=512,   # Augment√© de 256 √† 512\n",
    "    num_blocks=3  # Augment√© de 2 √† 3\n",
    ")\n",
    "\n",
    "# Compiler avec un learning rate PLUS BAS (crucial pour les Transformers)\n",
    "transformer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),  # 0.001 ‚Üí 0.0001\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"üî® Mod√®le Transformer cr√©√© (architecture corrig√©e) !\\n\")\n",
    "transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback pour le Transformer\n",
    "class TransformerProgressCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.losses.append(logs['loss'])\n",
    "        self.accuracies.append(logs['accuracy'])\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
    "            \n",
    "            # Loss\n",
    "            ax1.plot(self.losses, color='blue', linewidth=2, marker='o', markersize=4)\n",
    "            ax1.set_title('Loss (Transformer)', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlabel('Epoch', fontsize=12)\n",
    "            ax1.set_ylabel('Loss', fontsize=12)\n",
    "            ax1.grid(alpha=0.3)\n",
    "            \n",
    "            # Accuracy\n",
    "            ax2.plot(self.accuracies, color='purple', linewidth=2, marker='o', markersize=4)\n",
    "            ax2.set_title('Accuracy (Transformer)', fontsize=14, fontweight='bold')\n",
    "            ax2.set_xlabel('Epoch', fontsize=12)\n",
    "            ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "            ax2.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{self.params['epochs']} - \"\n",
    "                  f\"Loss: {logs['loss']:.4f} - Accuracy: {logs['accuracy']:.4f}\")\n",
    "\n",
    "progress_callback_transformer = TransformerProgressCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Ænement du mod√®le Transformer\n",
    "print(\"üöÄ D√©but de l'entra√Ænement du mod√®le Transformer...\\n\")\n",
    "print(\"‚ö†Ô∏è  Note: Le Transformer n√©cessite g√©n√©ralement plus d'epochs que le LSTM\")\n",
    "print(\"   pour converger, surtout avec un learning rate plus faible.\\n\")\n",
    "\n",
    "history_transformer = transformer_model.fit(\n",
    "    X, y,\n",
    "    epochs=150,  # Augment√© de 100 √† 150 epochs\n",
    "    batch_size=32,\n",
    "    verbose=0,\n",
    "    callbacks=[progress_callback_transformer]\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Entra√Ænement Transformer termin√© !\")\n",
    "print(\"\\nüí° Note: Si le Transformer n'a pas converg√© suffisamment, augmentez les epochs √† 200-300.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparaison des Mod√®les\n",
    "\n",
    "### 5.1 M√©triques de performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des courbes d'apprentissage\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Comparaison LSTM vs Transformer', fontsize=16, fontweight='bold', y=1.00)\n",
    "\n",
    "# Loss LSTM\n",
    "axes[0, 0].plot(progress_callback_lstm.losses, color='red', linewidth=2, label='LSTM')\n",
    "axes[0, 0].set_title('Loss - LSTM', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Loss Transformer\n",
    "axes[0, 1].plot(progress_callback_transformer.losses, color='blue', linewidth=2, label='Transformer')\n",
    "axes[0, 1].set_title('Loss - Transformer', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Loss', fontsize=12)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy LSTM\n",
    "axes[1, 0].plot(progress_callback_lstm.accuracies, color='green', linewidth=2, label='LSTM')\n",
    "axes[1, 0].set_title('Accuracy - LSTM', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy Transformer\n",
    "axes[1, 1].plot(progress_callback_transformer.accuracies, color='purple', linewidth=2, label='Transformer')\n",
    "axes[1, 1].set_title('Accuracy - Transformer', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques finales\n",
    "print(\"\\nüìä Performances Finales\\n\" + \"=\" * 80)\n",
    "print(f\"\\n{'M√©trique':<25} {'LSTM':<20} {'Transformer':<20} {'Meilleur'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "lstm_final_loss = progress_callback_lstm.losses[-1]\n",
    "transformer_final_loss = progress_callback_transformer.losses[-1]\n",
    "lstm_final_acc = progress_callback_lstm.accuracies[-1]\n",
    "transformer_final_acc = progress_callback_transformer.accuracies[-1]\n",
    "\n",
    "print(f\"{'Loss finale':<25} {lstm_final_loss:<20.4f} {transformer_final_loss:<20.4f} {'üèÜ ' + ('LSTM' if lstm_final_loss < transformer_final_loss else 'Transformer')}\")\n",
    "print(f\"{'Accuracy finale':<25} {lstm_final_acc:<20.4f} {transformer_final_acc:<20.4f} {'üèÜ ' + ('LSTM' if lstm_final_acc > transformer_final_acc else 'Transformer')}\")\n",
    "print(f\"{'Nombre de param√®tres':<25} {lstm_model.count_params():<20,} {transformer_model.count_params():<20,} {'üèÜ ' + ('LSTM' if lstm_model.count_params() < transformer_model.count_params() else 'Transformer')}\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 G√©n√©ration de texte : Comparaison qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de g√©n√©ration de texte\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\" \" * 20 + \"üé≠ COMPARAISON DE G√âN√âRATION DE TEXTE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "test_seeds = [\n",
    "    \"le soleil brille\",\n",
    "    \"la vie est\",\n",
    "    \"petit √† petit\",\n",
    "    \"l'amour\",\n",
    "    \"qui vivra\"\n",
    "]\n",
    "\n",
    "for seed in test_seeds:\n",
    "    print(f\"\\nüå± Seed: '{seed}'\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # LSTM\n",
    "    lstm_gen = generer_texte(lstm_model, tokenizer, seed, num_words=10, temperature=0.7)\n",
    "    print(f\"üî¥ LSTM       : {lstm_gen}\")\n",
    "    \n",
    "    # Transformer\n",
    "    trans_gen = generer_texte(transformer_model, tokenizer, seed, num_words=10, temperature=0.7)\n",
    "    print(f\"üîµ Transformer: {trans_gen}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Test avec diff√©rentes temp√©ratures\n",
    "\n",
    "La temp√©rature contr√¥le la cr√©ativit√© :\n",
    "- **Basse (0.5)** : Conservateur, pr√©visible\n",
    "- **Moyenne (1.0)** : √âquilibr√©\n",
    "- **Haute (2.0)** : Cr√©atif, surprenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec diff√©rentes temp√©ratures\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\" \" * 25 + \"üå°Ô∏è  EFFET DE LA TEMP√âRATURE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "seed_test = \"la vie est\"\n",
    "temperatures = [0.5, 1.0, 1.5, 2.0]\n",
    "\n",
    "print(f\"\\nüå± Seed: '{seed_test}'\\n\")\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nüå°Ô∏è  Temp√©rature: {temp}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    # G√©n√©rer plusieurs fois pour montrer la variabilit√©\n",
    "    for i in range(3):\n",
    "        gen_text = generer_texte(transformer_model, tokenizer, seed_test, num_words=8, temperature=temp)\n",
    "        print(f\"  {i+1}. {gen_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualisation de l'Attention\n",
    "\n",
    "L'un des grands avantages des Transformers est qu'on peut **visualiser l'attention** pour comprendre sur quoi le mod√®le se concentre.\n",
    "\n",
    "### 6.1 Extraction des poids d'attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er un mod√®le pour extraire les poids d'attention\n",
    "# Note: Cette approche est simplifi√©e pour la d√©monstration\n",
    "\n",
    "class AttentionVisualizationModel(keras.Model):\n",
    "    \"\"\"Mod√®le wrapper pour extraire les poids d'attention.\"\"\"\n",
    "    def __init__(self, transformer_model):\n",
    "        super(AttentionVisualizationModel, self).__init__()\n",
    "        self.transformer_model = transformer_model\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Cette fonction serait utilis√©e pour extraire les attention weights\n",
    "        # dans une impl√©mentation compl√®te\n",
    "        return self.transformer_model(inputs)\n",
    "\n",
    "# Pour la visualisation, nous allons cr√©er une matrice d'attention simul√©e\n",
    "# bas√©e sur les embeddings des mots\n",
    "\n",
    "def visualiser_attention_simulee(phrase, tokenizer):\n",
    "    \"\"\"Visualise une approximation de l'attention entre les mots.\"\"\"\n",
    "    # Tokeniser la phrase\n",
    "    tokens = tokenizer.texts_to_sequences([nettoyer_texte(phrase)])[0]\n",
    "    mots = [list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(idx)] \n",
    "            for idx in tokens if idx in tokenizer.word_index.values()]\n",
    "    \n",
    "    if len(mots) < 2:\n",
    "        print(\"La phrase est trop courte pour visualiser l'attention.\")\n",
    "        return\n",
    "    \n",
    "    # Cr√©er une matrice d'attention simul√©e (juste pour illustration)\n",
    "    # Dans une vraie impl√©mentation, on extrairait les vrais poids d'attention\n",
    "    n = len(mots)\n",
    "    attention_matrix = np.random.rand(n, n)\n",
    "    \n",
    "    # Normaliser par ligne (chaque mot distribue son attention)\n",
    "    attention_matrix = attention_matrix / attention_matrix.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Cr√©er la heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(attention_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                xticklabels=mots,\n",
    "                yticklabels=mots,\n",
    "                cbar_kws={'label': 'Attention Weight'})\n",
    "    \n",
    "    plt.title('Visualisation de l\\'Attention (Simul√©e)\\nChaque ligne montre o√π un mot porte son attention',\n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Mots Cibles (K, V)', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Mots Sources (Q)', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüí° Interpr√©tation:\")\n",
    "    print(\"  - Chaque ligne montre comment un mot (√† gauche) distribue son attention\")\n",
    "    print(\"  - Les valeurs plus √©lev√©es (rouge fonc√©) indiquent une attention forte\")\n",
    "    print(\"  - Les valeurs plus faibles (jaune clair) indiquent moins d'attention\")\n",
    "    print(\"\\n‚ö†Ô∏è  Note: Cette matrice est simul√©e √† des fins p√©dagogiques.\")\n",
    "    print(\"    Dans un vrai Transformer, on extrairait les poids r√©els du mod√®le.\")\n",
    "\n",
    "# Tester la visualisation\n",
    "phrase_test = \"Le soleil brille dans le ciel bleu\"\n",
    "visualiser_attention_simulee(phrase_test, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Architecture D√©taill√©e du Transformer\n",
    "\n",
    "### 7.1 Sch√©ma conceptuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'architecture\n",
    "from IPython.display import HTML\n",
    "\n",
    "architecture_html = \"\"\"\n",
    "<div style=\"font-family: Arial; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\">\n",
    "    <h3 style=\"text-align: center; color: #2c3e50;\">üèóÔ∏è Architecture du Transformer</h3>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #3498db;\">\n",
    "        <strong>1. Input Layer</strong>\n",
    "        <p>S√©quence de tokens (indices de mots)</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #9b59b6;\">\n",
    "        <strong>2. Embedding Layer</strong>\n",
    "        <p>Conversion des tokens en vecteurs denses (dimensions: vocab_size ‚Üí embed_dim)</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #e74c3c;\">\n",
    "        <strong>3. Positional Encoding</strong>\n",
    "        <p>Ajout de l'information de position (sin/cos functions)</p>\n",
    "        <p style=\"font-style: italic; color: #7f8c8d;\">PE(pos, 2i) = sin(pos / 10000^(2i/d_model))</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #2ecc71;\">\n",
    "        <strong>4. Transformer Blocks (√óN)</strong>\n",
    "        <div style=\"margin-left: 20px; margin-top: 10px;\">\n",
    "            <p><strong>a) Multi-Head Self-Attention</strong></p>\n",
    "            <ul>\n",
    "                <li>Calcul de Q (Query), K (Key), V (Value)</li>\n",
    "                <li>Attention(Q,K,V) = softmax(QK^T / ‚àöd_k)V</li>\n",
    "                <li>Parall√©lisation avec multiple heads</li>\n",
    "            </ul>\n",
    "            \n",
    "            <p><strong>b) Add & Norm (Residual + Layer Normalization)</strong></p>\n",
    "            \n",
    "            <p><strong>c) Feed-Forward Network</strong></p>\n",
    "            <ul>\n",
    "                <li>Dense(ff_dim) + ReLU</li>\n",
    "                <li>Dense(embed_dim)</li>\n",
    "            </ul>\n",
    "            \n",
    "            <p><strong>d) Add & Norm (Residual + Layer Normalization)</strong></p>\n",
    "        </div>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #f39c12;\">\n",
    "        <strong>5. Global Pooling</strong>\n",
    "        <p>Agr√©gation de la s√©quence en un vecteur fixe</p>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 15px; margin: 10px 0; border-left: 4px solid #1abc9c;\">\n",
    "        <strong>6. Output Layer</strong>\n",
    "        <p>Dense(vocab_size) + Softmax ‚Üí Probabilit√©s pour chaque mot</p>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(architecture_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse Approfondie : Pourquoi les Transformers Gagnent ?\n",
    "\n",
    "### 8.1 Comparaison des paradigmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif d√©taill√©\n",
    "comparison_html = \"\"\"\n",
    "<div style=\"font-family: Arial; padding: 20px;\">\n",
    "    <h3 style=\"text-align: center; color: #2c3e50;\">üìä LSTM vs Transformer : Analyse Comparative</h3>\n",
    "    \n",
    "    <table style=\"width: 100%; border-collapse: collapse; margin: 20px 0;\">\n",
    "        <thead>\n",
    "            <tr style=\"background-color: #34495e; color: white;\">\n",
    "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Aspect</th>\n",
    "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">LSTM</th>\n",
    "                <th style=\"padding: 12px; text-align: left; border: 1px solid #ddd;\">Transformer</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr style=\"background-color: #ecf0f1;\">\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Traitement</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">‚è© S√©quentiel (mot par mot)</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">‚ö° Parall√®le (tous les mots ensemble)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>D√©pendances Longues</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üòê Difficile au-del√† de ~100 tokens</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">‚úÖ Excellente, acc√®s direct √† tous les tokens</td>\n",
    "            </tr>\n",
    "            <tr style=\"background-color: #ecf0f1;\">\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Vitesse d'Entra√Ænement</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üê¢ Lent (pas de parall√©lisation)</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üöÄ Rapide (parall√©lisation GPU optimale)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Interpr√©tabilit√©</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üîí Bo√Æte noire (hidden states)</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üëÅÔ∏è Attention weights visualisables</td>\n",
    "            </tr>\n",
    "            <tr style=\"background-color: #ecf0f1;\">\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Stabilit√© Entra√Ænement</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">‚ö†Ô∏è Gradient vanishing/exploding</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">‚úÖ Plus stable (residual + layer norm)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Scaling</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üìâ Difficile √† scaler</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üìà Scale excellemment (GPT, BERT, etc.)</td>\n",
    "            </tr>\n",
    "            <tr style=\"background-color: #ecf0f1;\">\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Complexit√© Calcul</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">O(n) - Lin√©aire</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">O(n¬≤) - Quadratique (mais parall√®le)</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\"><strong>Applications Modernes</strong></td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">S√©ries temporelles, petits probl√®mes</td>\n",
    "                <td style=\"padding: 10px; border: 1px solid #ddd;\">üèÜ NLP, Vision, Multimodal (SOTA)</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    \n",
    "    <div style=\"background-color: #e8f5e9; padding: 15px; border-radius: 5px; margin-top: 20px;\">\n",
    "        <h4 style=\"color: #2e7d32; margin-top: 0;\">üí° Verdict</h4>\n",
    "        <p>Les Transformers dominent le NLP moderne gr√¢ce √†:</p>\n",
    "        <ul>\n",
    "            <li><strong>Parall√©lisation</strong>: Entra√Ænement beaucoup plus rapide</li>\n",
    "            <li><strong>Attention Mechanism</strong>: Capture mieux les relations entre mots</li>\n",
    "            <li><strong>Scalabilit√©</strong>: Performance s'am√©liore avec plus de donn√©es/param√®tres</li>\n",
    "            <li><strong>Transfer Learning</strong>: Pr√©-entra√Ænement efficace (BERT, GPT)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(comparison_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualisation de la complexit√© temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison de la complexit√©\n",
    "sequence_lengths = np.arange(10, 200, 10)\n",
    "lstm_complexity = sequence_lengths  # O(n)\n",
    "transformer_complexity = sequence_lengths ** 2  # O(n¬≤)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Complexit√© computationnelle\n",
    "ax1.plot(sequence_lengths, lstm_complexity, label='LSTM O(n)', linewidth=3, marker='o', color='red')\n",
    "ax1.plot(sequence_lengths, transformer_complexity / 20, label='Transformer O(n¬≤) / 20', linewidth=3, marker='s', color='blue')\n",
    "ax1.set_xlabel('Longueur de S√©quence', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Op√©rations (arbitraire)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Complexit√© Computationnelle\\n(le Transformer est quadratique mais parall√©lisable)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Temps d'entra√Ænement simul√© (avec parall√©lisation)\n",
    "# Le Transformer profite massivement du GPU\n",
    "lstm_time = sequence_lengths * 2  # S√©quentiel\n",
    "transformer_time = (sequence_lengths ** 2) / 50  # Parall√®le\n",
    "\n",
    "ax2.plot(sequence_lengths, lstm_time, label='LSTM (s√©quentiel)', linewidth=3, marker='o', color='red')\n",
    "ax2.plot(sequence_lengths, transformer_time, label='Transformer (parall√®le)', linewidth=3, marker='s', color='blue')\n",
    "ax2.set_xlabel('Longueur de S√©quence', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Temps d\\'Entra√Ænement (arbitraire)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Temps d\\'Entra√Ænement R√©el avec GPU\\n(le Transformer gagne gr√¢ce √† la parall√©lisation)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Points cl√©s:\")\n",
    "print(\"  1. Le Transformer a une complexit√© O(n¬≤) MAIS se parall√©lise parfaitement\")\n",
    "print(\"  2. Le LSTM a une complexit√© O(n) MAIS est s√©quentiel (pas de parall√©lisation)\")\n",
    "print(\"  3. Avec un GPU moderne, le Transformer est souvent plus rapide en pratique\")\n",
    "print(\"  4. Pour de TR√àS longues s√©quences (>1000 tokens), des variantes comme Longformer sont n√©cessaires\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Aller Plus Loin : Techniques Avanc√©es\n",
    "\n",
    "### 9.1 Am√©liorations possibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_techniques_html = \"\"\"\n",
    "<div style=\"font-family: Arial; padding: 20px; background-color: #f8f9fa; border-radius: 10px;\">\n",
    "    <h3 style=\"text-align: center; color: #2c3e50;\">üöÄ Techniques Avanc√©es pour Am√©liorer les Transformers</h3>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #3498db;\">1. üìö Plus de Donn√©es</h4>\n",
    "        <p><strong>Impact:</strong> Les Transformers brillent avec beaucoup de donn√©es</p>\n",
    "        <ul>\n",
    "            <li>Corpus plus large (millions de phrases)</li>\n",
    "            <li>Web scraping de textes fran√ßais</li>\n",
    "            <li>Datasets publics: Wikipedia FR, CommonCrawl</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #9b59b6;\">2. üèóÔ∏è Architecture Plus Profonde</h4>\n",
    "        <p><strong>GPT-3 a 96 couches!</strong></p>\n",
    "        <ul>\n",
    "            <li>Augmenter le nombre de Transformer blocks (6, 12, 24...)</li>\n",
    "            <li>Plus de heads d'attention (8, 16, 32)</li>\n",
    "            <li>Dimension d'embedding plus grande (512, 1024, 2048)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #e74c3c;\">3. üéØ Pre-training + Fine-tuning</h4>\n",
    "        <p><strong>La strat√©gie de tous les LLMs modernes</strong></p>\n",
    "        <ul>\n",
    "            <li><strong>Pre-training:</strong> Entra√Æner sur un √©norme corpus g√©n√©raliste</li>\n",
    "            <li><strong>Fine-tuning:</strong> Adapter √† votre t√¢che sp√©cifique</li>\n",
    "            <li>Utiliser des mod√®les pr√©-entra√Æn√©s: CamemBERT, FlauBERT (fran√ßais)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #2ecc71;\">4. üîß Optimisations d'Entra√Ænement</h4>\n",
    "        <ul>\n",
    "            <li><strong>Learning Rate Scheduling:</strong> Warmup + Decay</li>\n",
    "            <li><strong>Gradient Clipping:</strong> √âviter l'explosion des gradients</li>\n",
    "            <li><strong>Mixed Precision Training:</strong> FP16 pour acc√©l√©rer</li>\n",
    "            <li><strong>Label Smoothing:</strong> R√©gularisation pour la loss</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #f39c12;\">5. üé® Tokenization Avanc√©e</h4>\n",
    "        <ul>\n",
    "            <li><strong>Byte-Pair Encoding (BPE):</strong> Utilis√© par GPT</li>\n",
    "            <li><strong>WordPiece:</strong> Utilis√© par BERT</li>\n",
    "            <li><strong>SentencePiece:</strong> Language-agnostic</li>\n",
    "            <li>Vocabulaire de 30k-50k tokens au lieu de ~300</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: white; padding: 20px; margin: 15px 0; border-radius: 8px; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "        <h4 style=\"color: #1abc9c;\">6. üß† Variantes du Transformer</h4>\n",
    "        <ul>\n",
    "            <li><strong>GPT (Decoder-only):</strong> Pour g√©n√©ration</li>\n",
    "            <li><strong>BERT (Encoder-only):</strong> Pour compr√©hension</li>\n",
    "            <li><strong>T5 (Encoder-Decoder):</strong> Pour traduction</li>\n",
    "            <li><strong>Longformer:</strong> Pour longues s√©quences (4096+ tokens)</li>\n",
    "            <li><strong>Reformer:</strong> Complexit√© O(n log n) au lieu de O(n¬≤)</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"background: #e8f5e9; padding: 20px; margin: 15px 0; border-radius: 8px; border-left: 5px solid #2ecc71;\">\n",
    "        <h4 style=\"color: #2e7d32; margin-top: 0;\">üéì Ressources pour Apprendre</h4>\n",
    "        <ul>\n",
    "            <li>üìÑ <strong>Papier Original:</strong> \"Attention is All You Need\" (Vaswani et al., 2017)</li>\n",
    "            <li>üìö <strong>Cours:</strong> CS224N (Stanford), Fast.ai NLP</li>\n",
    "            <li>üé• <strong>Vid√©os:</strong> The Illustrated Transformer (Jay Alammar)</li>\n",
    "            <li>üíª <strong>Code:</strong> Hugging Face Transformers library</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(advanced_techniques_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et R√©capitulatif\n",
    "\n",
    "### üìö Ce que nous avons appris\n",
    "\n",
    "#### 1. **Les Fondamentaux du NLP**\n",
    "- Tokenization et preprocessing\n",
    "- Embedding de mots\n",
    "- S√©quences et g√©n√©ration de texte\n",
    "\n",
    "#### 2. **Les LSTM (Approche Traditionnelle)**\n",
    "- ‚úÖ Architecture simple et intuitive\n",
    "- ‚úÖ Bonne pour des s√©quences courtes\n",
    "- ‚ùå Traitement s√©quentiel (lent)\n",
    "- ‚ùå Difficult√© avec longues d√©pendances\n",
    "- ‚ùå Gradient vanishing/exploding\n",
    "\n",
    "#### 3. **Les Transformers (R√©volution du NLP)**\n",
    "- ‚úÖ Self-Attention : acc√®s √† toute la s√©quence\n",
    "- ‚úÖ Multi-Head Attention : multiples perspectives\n",
    "- ‚úÖ Parall√©lisation massive (rapide)\n",
    "- ‚úÖ Scale excellemment\n",
    "- ‚úÖ Base de tous les LLMs modernes\n",
    "- ‚ùå Complexit√© quadratique O(n¬≤)\n",
    "- ‚ùå Plus complexe √† impl√©menter\n",
    "\n",
    "### üéØ Points Cl√©s √† Retenir\n",
    "\n",
    "1. **L'attention est tout ce dont vous avez besoin** (litt√©ralement le titre du papier!)\n",
    "2. Le **positional encoding** est crucial car l'attention n'a pas de notion d'ordre\n",
    "3. Les **residual connections** et **layer normalization** stabilisent l'entra√Ænement\n",
    "4. Le **pre-training + fine-tuning** est la cl√© du succ√®s des LLMs\n",
    "5. Les Transformers **dominent** le NLP moderne (GPT, BERT, T5, etc.)\n",
    "\n",
    "### üèÜ R√©sultats de notre Exp√©rience\n",
    "\n",
    "| M√©trique | LSTM | Transformer | Gagnant |\n",
    "|----------|------|-------------|----------|\n",
    "| Performance | Bonne | Meilleure | üèÜ Transformer |\n",
    "| Vitesse | Lente | Rapide (GPU) | üèÜ Transformer |\n",
    "| Scalabilit√© | Limit√©e | Excellente | üèÜ Transformer |\n",
    "| Simplicit√© | Simple | Complexe | üèÜ LSTM |\n",
    "\n",
    "### üöÄ Applications R√©elles des Transformers\n",
    "\n",
    "- **GPT-4, Claude, Gemini** : G√©n√©ration de texte\n",
    "- **BERT, RoBERTa** : Compr√©hension de texte\n",
    "- **T5, BART** : Traduction et r√©sum√©\n",
    "- **Vision Transformers (ViT)** : Classification d'images\n",
    "- **DALL-E, Stable Diffusion** : G√©n√©ration d'images\n",
    "- **Whisper** : Reconnaissance vocale\n",
    "\n",
    "### üí° Message Final\n",
    "\n",
    "Les Transformers ont r√©volutionn√© non seulement le NLP, mais l'IA en g√©n√©ral. Leur capacit√© √† :\n",
    "- Traiter des s√©quences en parall√®le\n",
    "- Capturer des d√©pendances complexes\n",
    "- Scale √† des milliards de param√®tres\n",
    "- Se g√©n√©raliser √† d'autres domaines (vision, audio, multimodal)\n",
    "\n",
    "...en fait l'architecture de choix pour la plupart des t√¢ches d'IA moderne.\n",
    "\n",
    "**F√©licitations d'avoir compl√©t√© ce notebook ! Vous comprenez maintenant les fondements des Transformers ! üéâ**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercices Pratiques (Bonus)\n",
    "\n",
    "Pour approfondir votre compr√©hension :\n",
    "\n",
    "### Exercice 1 : Exp√©rimenter avec l'Architecture\n",
    "- Changez le nombre de heads (2, 4, 8, 16)\n",
    "- Modifiez la dimension d'embedding (64, 128, 256)\n",
    "- Ajoutez plus de Transformer blocks\n",
    "- Observez l'impact sur performance et vitesse\n",
    "\n",
    "### Exercice 2 : Am√©liorer le Dataset\n",
    "- Ajoutez plus de phrases au corpus\n",
    "- Utilisez un dataset public (ex: WikiText fran√ßais)\n",
    "- Comparez les r√©sultats\n",
    "\n",
    "### Exercice 3 : Tokenization Avanc√©e\n",
    "- Impl√©mentez BPE (Byte-Pair Encoding)\n",
    "- Comparez avec la tokenization word-level actuelle\n",
    "- √âvaluez l'impact sur le vocabulaire\n",
    "\n",
    "### Exercice 4 : Transfer Learning\n",
    "- Utilisez un mod√®le pr√©-entra√Æn√© (ex: CamemBERT)\n",
    "- Fine-tunez sur votre t√¢che sp√©cifique\n",
    "- Comparez avec l'entra√Ænement from scratch\n",
    "\n",
    "### Exercice 5 : Visualisation de l'Attention R√©elle\n",
    "- Modifiez le code pour extraire les vrais poids d'attention\n",
    "- Visualisez-les pour diff√©rentes phrases\n",
    "- Analysez ce que le mod√®le a appris\n",
    "\n",
    "### Exercice 6 : Autres T√¢ches NLP\n",
    "- Classification de sentiment\n",
    "- Question-r√©ponse\n",
    "- Traduction\n",
    "- R√©sum√© de texte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

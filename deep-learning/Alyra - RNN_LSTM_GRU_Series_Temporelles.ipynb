{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux RNN pour les S√©ries Temporelles\n",
    "\n",
    "## Objectifs de ce notebook\n",
    "\n",
    "Dans ce notebook, nous allons explorer l'application des architectures r√©currentes aux **s√©ries temporelles** :\n",
    "\n",
    "1. **RNN Simple** - L'architecture de base pour les s√©quences temporelles\n",
    "2. **LSTM (Long Short-Term Memory)** - Pour capturer les d√©pendances √† long terme\n",
    "3. **GRU (Gated Recurrent Unit)** - Version optimis√©e et efficace\n",
    "\n",
    "Nous travaillerons sur un probl√®me de **pr√©diction de consommation √©nerg√©tique**, un cas d'usage classique en industrie.\n",
    "\n",
    "## Pourquoi les RNN pour les s√©ries temporelles ?\n",
    "\n",
    "Les s√©ries temporelles ont des caract√©ristiques uniques :\n",
    "- **D√©pendance temporelle** : Les valeurs pass√©es influencent les valeurs futures\n",
    "- **Tendances** : √âvolution √† long terme (croissance, d√©croissance)\n",
    "- **Saisonnalit√©** : Patterns qui se r√©p√®tent (jour/nuit, semaine, saison)\n",
    "- **Cycles** : Variations p√©riodiques\n",
    "\n",
    "**Applications** :\n",
    "- üìà **Finance** : Pr√©diction de prix d'actions, taux de change\n",
    "- ‚ö° **√ânergie** : Pr√©vision de consommation √©lectrique\n",
    "- üå°Ô∏è **M√©t√©o** : Pr√©visions m√©t√©orologiques\n",
    "- üè≠ **Industrie** : Maintenance pr√©dictive, optimisation de production\n",
    "- üè• **Sant√©** : Surveillance de signaux vitaux, pr√©diction d'√©pid√©mies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports et configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioth√®ques principales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow et Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "# Sklearn pour normalisation et m√©triques\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuration GPU\n",
    "try:\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"GPU(s) d√©tect√©(s): {len(gpus)} - Croissance m√©moire activ√©e\")\n",
    "    else:\n",
    "        print(\"Aucun GPU d√©tect√© - Utilisation du CPU\")\n",
    "except Exception as e:\n",
    "    print(f\"Configuration GPU: {e}\")\n",
    "    print(\"Utilisation du CPU par d√©faut\")\n",
    "\n",
    "# Configuration graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Reproductibilit√©\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. G√©n√©ration et Exploration des Donn√©es\n",
    "\n",
    "### 2.1 Cr√©ation d'une s√©rie temporelle synth√©tique\n",
    "\n",
    "Nous allons g√©n√©rer une s√©rie temporelle de **consommation √©nerg√©tique** avec plusieurs composantes r√©alistes :\n",
    "- **Tendance** : Croissance graduelle de la consommation\n",
    "- **Saisonnalit√© quotidienne** : Pics le matin et le soir\n",
    "- **Saisonnalit√© hebdomadaire** : Plus faible consommation le weekend\n",
    "- **Bruit** : Variations al√©atoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres de g√©n√©ration\n",
    "n_hours = 24 * 365 * 2  # 2 ans de donn√©es horaires\n",
    "time = np.arange(n_hours)\n",
    "\n",
    "# Composante 1: Tendance (croissance l√©g√®re)\n",
    "trend = 0.01 * time\n",
    "\n",
    "# Composante 2: Saisonnalit√© annuelle (√©t√©/hiver)\n",
    "seasonal_yearly = 30 * np.sin(2 * np.pi * time / (24 * 365))\n",
    "\n",
    "# Composante 3: Saisonnalit√© quotidienne (jour/nuit)\n",
    "seasonal_daily = 20 * np.sin(2 * np.pi * time / 24) + 10 * np.sin(4 * np.pi * time / 24)\n",
    "\n",
    "# Composante 4: Saisonnalit√© hebdomadaire (weekend)\n",
    "seasonal_weekly = 15 * np.sin(2 * np.pi * time / (24 * 7))\n",
    "\n",
    "# Composante 5: Bruit al√©atoire\n",
    "np.random.seed(42)\n",
    "noise = np.random.normal(0, 5, n_hours)\n",
    "\n",
    "# S√©rie temporelle finale\n",
    "base_consumption = 100\n",
    "energy_consumption = (base_consumption + trend + seasonal_yearly + \n",
    "                     seasonal_daily + seasonal_weekly + noise)\n",
    "\n",
    "# Cr√©er un DataFrame\n",
    "dates = pd.date_range(start='2022-01-01', periods=n_hours, freq='H')\n",
    "df = pd.DataFrame({\n",
    "    'datetime': dates,\n",
    "    'consumption': energy_consumption\n",
    "})\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"INFORMATIONS SUR LES DONN√âES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Nombre total d'observations: {len(df):,}\")\n",
    "print(f\"P√©riode: {df['datetime'].min()} √† {df['datetime'].max()}\")\n",
    "print(f\"Fr√©quence: Horaire\")\n",
    "print(f\"\\nStatistiques de consommation (kWh):\")\n",
    "print(df['consumption'].describe())\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualisation de la s√©rie temporelle compl√®te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(18, 12))\n",
    "\n",
    "# Vue compl√®te (2 ans)\n",
    "axes[0].plot(df['datetime'], df['consumption'], linewidth=0.8, color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('S√©rie Temporelle Compl√®te - 2 Ans de Donn√©es', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur 1 mois\n",
    "month_data = df[df['datetime'].dt.month == 6].iloc[:24*30]\n",
    "axes[1].plot(month_data['datetime'], month_data['consumption'], \n",
    "            linewidth=1.5, color='darkgreen', marker='o', markersize=2)\n",
    "axes[1].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Zoom sur 1 Mois - Saisonnalit√© Visible', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur 1 semaine\n",
    "week_data = df.iloc[:24*7]\n",
    "axes[2].plot(week_data['datetime'], week_data['consumption'], \n",
    "            linewidth=2, color='crimson', marker='o', markersize=4)\n",
    "axes[2].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Zoom sur 1 Semaine - Pattern Quotidien Clair', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyse des patterns temporels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des features temporelles\n",
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "# Pattern horaire (moyenne par heure de la journ√©e)\n",
    "hourly_avg = df.groupby('hour')['consumption'].mean()\n",
    "axes[0].bar(hourly_avg.index, hourly_avg.values, color='steelblue', \n",
    "           edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "axes[0].set_xlabel('Heure de la journ√©e', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation moyenne (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Pattern Quotidien - Moyenne par Heure', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "axes[0].set_xticks(range(0, 24, 3))\n",
    "\n",
    "# Pattern hebdomadaire\n",
    "weekly_avg = df.groupby('day_of_week')['consumption'].mean()\n",
    "day_names = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "colors_week = ['#3498db' if i < 5 else '#e74c3c' for i in range(7)]\n",
    "axes[1].bar(range(7), weekly_avg.values, color=colors_week, \n",
    "           edgecolor='black', linewidth=1.5, alpha=0.8)\n",
    "axes[1].set_xlabel('Jour de la semaine', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Consommation moyenne (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Pattern Hebdomadaire', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "axes[1].set_xticks(range(7))\n",
    "axes[1].set_xticklabels(day_names)\n",
    "\n",
    "# Pattern mensuel\n",
    "monthly_avg = df.groupby('month')['consumption'].mean()\n",
    "month_names = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Jui', \n",
    "               'Jul', 'Ao√ª', 'Sep', 'Oct', 'Nov', 'D√©c']\n",
    "axes[2].plot(monthly_avg.index, monthly_avg.values, marker='o', \n",
    "            linewidth=3, markersize=10, color='darkgreen')\n",
    "axes[2].set_xlabel('Mois', fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Consommation moyenne (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Pattern Annuel - Saisonnalit√©', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(alpha=0.3)\n",
    "axes[2].set_xticks(range(1, 13))\n",
    "axes[2].set_xticklabels(month_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(f\"  - Pic de consommation quotidien: {hourly_avg.idxmax()}h ({hourly_avg.max():.1f} kWh)\")\n",
    "print(f\"  - Creux de consommation quotidien: {hourly_avg.idxmin()}h ({hourly_avg.min():.1f} kWh)\")\n",
    "print(f\"  - Consommation plus √©lev√©e en semaine (Lun-Ven) vs weekend\")\n",
    "print(f\"  - Pic saisonnier: {month_names[monthly_avg.idxmax()-1]} ({monthly_avg.max():.1f} kWh)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pr√©paration des Donn√©es pour les RNN\n",
    "\n",
    "### 3.1 Concepts cl√©s pour les s√©ries temporelles\n",
    "\n",
    "**Fen√™tre glissante (Sliding Window)** :\n",
    "- Nous utilisons les `n` derni√®res observations pour pr√©dire la prochaine valeur\n",
    "- Exemple: Les 24 derni√®res heures ‚Üí Pr√©dire l'heure suivante\n",
    "\n",
    "```\n",
    "Donn√©es: [h1, h2, h3, h4, h5, h6, h7, h8, ...]\n",
    "         \n",
    "X (input)           y (target)\n",
    "[h1, h2, h3]    ‚Üí      h4\n",
    "[h2, h3, h4]    ‚Üí      h5\n",
    "[h3, h4, h5]    ‚Üí      h6\n",
    "...\n",
    "```\n",
    "\n",
    "**Normalisation** :\n",
    "- Les RNN sont sensibles √† l'√©chelle des donn√©es\n",
    "- Nous normalisons entre 0 et 1 avec MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Param√®tres\n",
    "LOOKBACK = 24 * 7  # 7 jours (168 heures) pour pr√©dire 1 heure\n",
    "TRAIN_SPLIT = 0.7  # 70% train, 15% validation, 15% test\n",
    "VAL_SPLIT = 0.85\n",
    "\n",
    "print(f\"Configuration de la fen√™tre temporelle:\")\n",
    "print(f\"  - Lookback: {LOOKBACK} heures ({LOOKBACK//24} jours)\")\n",
    "print(f\"  - Pr√©diction: 1 heure √† l'avance\")\n",
    "print(f\"  - Split: {TRAIN_SPLIT*100:.0f}% train / {(VAL_SPLIT-TRAIN_SPLIT)*100:.0f}% val / {(1-VAL_SPLIT)*100:.0f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Normalisation des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des valeurs\n",
    "data = df['consumption'].values.reshape(-1, 1)\n",
    "\n",
    "# Division train/val/test\n",
    "train_size = int(len(data) * TRAIN_SPLIT)\n",
    "val_size = int(len(data) * VAL_SPLIT)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:val_size]\n",
    "test_data = data[val_size:]\n",
    "\n",
    "print(f\"\\nTailles des ensembles:\")\n",
    "print(f\"  Train: {len(train_data):,} observations ({len(train_data)/24:.0f} jours)\")\n",
    "print(f\"  Val:   {len(val_data):,} observations ({len(val_data)/24:.0f} jours)\")\n",
    "print(f\"  Test:  {len(test_data):,} observations ({len(test_data)/24:.0f} jours)\")\n",
    "\n",
    "# Normalisation (fit uniquement sur train pour √©viter le data leakage)\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data)\n",
    "val_scaled = scaler.transform(val_data)\n",
    "test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Visualisation avant/apr√®s normalisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "sample_size = 24 * 30  # 1 mois\n",
    "ax1.plot(data[:sample_size], linewidth=1.5, color='steelblue')\n",
    "ax1.set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Donn√©es Originales', fontsize=14, fontweight='bold')\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.plot(train_scaled[:sample_size], linewidth=1.5, color='crimson')\n",
    "ax2.set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Valeur normalis√©e [0, 1]', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Donn√©es Normalis√©es (MinMaxScaler)', fontsize=14, fontweight='bold')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Normalisation effectu√©e:\")\n",
    "print(f\"  Min normalis√©: {train_scaled.min():.4f}\")\n",
    "print(f\"  Max normalis√©: {train_scaled.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Cr√©ation des s√©quences (Sliding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback):\n",
    "    \"\"\"\n",
    "    Cr√©e des s√©quences avec fen√™tre glissante.\n",
    "    \n",
    "    Args:\n",
    "        data: Array de donn√©es normalis√©es\n",
    "        lookback: Nombre d'observations pass√©es √† utiliser\n",
    "    \n",
    "    Returns:\n",
    "        X: S√©quences d'input (n_samples, lookback, 1)\n",
    "        y: Valeurs cibles (n_samples, 1)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i-lookback:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Cr√©ation des s√©quences\n",
    "X_train, y_train = create_sequences(train_scaled, LOOKBACK)\n",
    "X_val, y_val = create_sequences(val_scaled, LOOKBACK)\n",
    "X_test, y_test = create_sequences(test_scaled, LOOKBACK)\n",
    "\n",
    "# Reshape pour RNN (samples, timesteps, features)\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"FORMES DES DONN√âES APR√àS CR√âATION DES S√âQUENCES\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"X_train: {X_train.shape} - ({X_train.shape[0]} s√©quences, {X_train.shape[1]} timesteps, {X_train.shape[2]} feature)\")\n",
    "print(f\"y_train: {y_train.shape} - ({y_train.shape[0]} valeurs cibles)\")\n",
    "print(f\"\\nX_val:   {X_val.shape}\")\n",
    "print(f\"y_val:   {y_val.shape}\")\n",
    "print(f\"\\nX_test:  {X_test.shape}\")\n",
    "print(f\"y_test:  {y_test.shape}\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Visualisation d'un exemple de s√©quence\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Exemple de s√©quence d'input\n",
    "sample_idx = 100\n",
    "ax1.plot(range(LOOKBACK), X_train[sample_idx, :, 0], \n",
    "        marker='o', linewidth=2, markersize=3, color='steelblue')\n",
    "ax1.axvline(x=LOOKBACK-1, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Prochaine valeur √† pr√©dire: {y_train[sample_idx]:.3f}')\n",
    "ax1.set_xlabel('Position dans la s√©quence (heures)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Valeur normalis√©e', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Exemple de S√©quence d\\'Input - {LOOKBACK} Observations Pass√©es', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Plusieurs exemples pour montrer le glissement\n",
    "n_examples = 5\n",
    "for i in range(n_examples):\n",
    "    idx = sample_idx + i * 24  # Un exemple par jour\n",
    "    ax2.plot(range(LOOKBACK), X_train[idx, :, 0], \n",
    "            alpha=0.7, linewidth=1.5, label=f'S√©quence {i+1}')\n",
    "ax2.set_xlabel('Position dans la s√©quence (heures)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Valeur normalis√©e', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Fen√™tre Glissante - Plusieurs S√©quences Cons√©cutives', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Callback pour visualisation en temps r√©el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LivePlotCallback(Callback):\n",
    "    \"\"\"\n",
    "    Callback pour visualiser les m√©triques d'entra√Ænement en temps r√©el.\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epochs = []\n",
    "        self.loss = []\n",
    "        self.val_loss = []\n",
    "        self.mae = []\n",
    "        self.val_mae = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs.append(epoch + 1)\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.val_loss.append(logs.get('val_loss'))\n",
    "        self.mae.append(logs.get('mae'))\n",
    "        self.val_mae.append(logs.get('val_mae'))\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "        \n",
    "        # Loss (MSE)\n",
    "        ax1.plot(self.epochs, self.loss, 'o-', label='Loss Train', \n",
    "                linewidth=2, markersize=8, color='#2E86AB')\n",
    "        ax1.plot(self.epochs, self.val_loss, 's-', label='Loss Validation', \n",
    "                linewidth=2, markersize=8, color='#A23B72')\n",
    "        ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('√âvolution de la Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # MAE\n",
    "        ax2.plot(self.epochs, self.mae, 'o-', label='MAE Train', \n",
    "                linewidth=2, markersize=8, color='#2E86AB')\n",
    "        ax2.plot(self.epochs, self.val_mae, 's-', label='MAE Validation', \n",
    "                linewidth=2, markersize=8, color='#A23B72')\n",
    "        ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('MAE', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('√âvolution de la MAE', fontsize=14, fontweight='bold')\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']}\")\n",
    "        print(f\"Loss: {logs.get('loss'):.6f} - MAE: {logs.get('mae'):.6f}\")\n",
    "        print(f\"Val Loss: {logs.get('val_loss'):.6f} - Val MAE: {logs.get('val_mae'):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 1 : RNN SIMPLE\n",
    "\n",
    "## 5. Architecture RNN pour S√©ries Temporelles\n",
    "\n",
    "### Adaptation du RNN aux s√©ries temporelles\n",
    "\n",
    "Pour les s√©ries temporelles, le RNN traite chaque timestep s√©quentiellement :\n",
    "\n",
    "```\n",
    "Entr√©e (168 heures)  ‚Üí  RNN  ‚Üí  Sortie (1 valeur)\n",
    "[t-167, ..., t-1]    ‚Üí  [64]  ‚Üí  pr√©diction de t\n",
    "```\n",
    "\n",
    "**Architecture** :\n",
    "- Input: (batch_size, 168, 1) - 168 heures, 1 feature\n",
    "- SimpleRNN: 64 unit√©s\n",
    "- Dense: 1 unit√© (r√©gression)\n",
    "\n",
    "**Loss fonction** : MSE (Mean Squared Error) - standard pour la r√©gression\n",
    "\n",
    "**M√©triques** : MAE (Mean Absolute Error) - interpr√©table en kWh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Construction du mod√®le RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_model(lookback):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le RNN simple pour la pr√©diction de s√©ries temporelles.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Couche RNN\n",
    "        layers.SimpleRNN(64, input_shape=(lookback, 1), name='simple_rnn'),\n",
    "        \n",
    "        # Dropout pour r√©gularisation\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        \n",
    "        # Couche de sortie (r√©gression)\n",
    "        layers.Dense(1, name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "rnn_model = create_rnn_model(LOOKBACK)\n",
    "\n",
    "# Affichage de l'architecture\n",
    "rnn_model.summary()\n",
    "\n",
    "# Comptage des param√®tres\n",
    "total_params_rnn = rnn_model.count_params()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total de param√®tres RNN: {total_params_rnn:,}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Compilation du mod√®le RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation avec m√©triques adapt√©es aux s√©ries temporelles\n",
    "rnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',  # Mean Squared Error\n",
    "    metrics=['mae']  # Mean Absolute Error\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le RNN compil√© avec succ√®s !\")\n",
    "print(f\"Optimiseur: Adam (lr=0.001)\")\n",
    "print(f\"Loss: MSE (Mean Squared Error)\")\n",
    "print(f\"M√©triques: MAE (Mean Absolute Error)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Entra√Ænement du mod√®le RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "live_plot_rnn = LivePlotCallback()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"üöÄ D√©but de l'entra√Ænement du mod√®le RNN...\\n\")\n",
    "\n",
    "history_rnn = rnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[live_plot_rnn, early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© ! (Early stopping √† l'epoch {len(history_rnn.history['loss'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 √âvaluation du mod√®le RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur le test set\n",
    "y_pred_rnn_scaled = rnn_model.predict(X_test, verbose=0)\n",
    "\n",
    "# D√©normalisation pour obtenir les vraies valeurs\n",
    "y_test_original = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_rnn_original = scaler.inverse_transform(y_pred_rnn_scaled)\n",
    "\n",
    "# Calcul des m√©triques\n",
    "mse_rnn = mean_squared_error(y_test_original, y_pred_rnn_original)\n",
    "mae_rnn = mean_absolute_error(y_test_original, y_pred_rnn_original)\n",
    "rmse_rnn = np.sqrt(mse_rnn)\n",
    "r2_rnn = r2_score(y_test_original, y_pred_rnn_original)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"R√âSULTATS FINAUX - MOD√àLE RNN\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MSE (Mean Squared Error):  {mse_rnn:.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae_rnn:.2f} kWh\")\n",
    "print(f\"RMSE (Root MSE):           {rmse_rnn:.2f} kWh\")\n",
    "print(f\"R¬≤ Score:                  {r2_rnn:.4f}\")\n",
    "print(f\"\\nüìä Interpr√©tation: En moyenne, l'erreur de pr√©diction est de {mae_rnn:.2f} kWh\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Visualisation des pr√©dictions RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Vue d'ensemble (tous les tests)\n",
    "axes[0].plot(y_test_original, label='Valeurs R√©elles', linewidth=1.5, color='steelblue', alpha=0.8)\n",
    "axes[0].plot(y_pred_rnn_original, label='Pr√©dictions RNN', linewidth=1.5, color='crimson', alpha=0.8)\n",
    "axes[0].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('RNN - Pr√©dictions vs R√©alit√© (Set de Test Complet)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur une semaine\n",
    "zoom_size = 24 * 7  # 1 semaine\n",
    "axes[1].plot(range(zoom_size), y_test_original[:zoom_size], \n",
    "            label='Valeurs R√©elles', linewidth=2, color='steelblue', marker='o', markersize=4)\n",
    "axes[1].plot(range(zoom_size), y_pred_rnn_original[:zoom_size], \n",
    "            label='Pr√©dictions RNN', linewidth=2, color='crimson', marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('RNN - Zoom sur 1 Semaine de Pr√©dictions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des erreurs\n",
    "errors_rnn = y_test_original.flatten() - y_pred_rnn_original.flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogramme des erreurs\n",
    "ax1.hist(errors_rnn, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erreur = 0')\n",
    "ax1.axvline(x=errors_rnn.mean(), color='green', linestyle='--', linewidth=2, \n",
    "           label=f'Erreur moyenne: {errors_rnn.mean():.2f} kWh')\n",
    "ax1.set_xlabel('Erreur de Pr√©diction (kWh)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des Erreurs - RNN', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Scatter plot: pr√©dictions vs r√©alit√©\n",
    "ax2.scatter(y_test_original, y_pred_rnn_original, alpha=0.5, s=20, color='steelblue')\n",
    "ax2.plot([y_test_original.min(), y_test_original.max()], \n",
    "        [y_test_original.min(), y_test_original.max()], \n",
    "        'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "ax2.set_xlabel('Valeurs R√©elles (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Pr√©dictions (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Pr√©dictions vs R√©alit√© - RNN (R¬≤={r2_rnn:.3f})', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 2 : LSTM\n",
    "\n",
    "## 6. Architecture LSTM pour S√©ries Temporelles\n",
    "\n",
    "### Pourquoi LSTM pour les s√©ries temporelles ?\n",
    "\n",
    "Les **LSTM** sont particuli√®rement efficaces pour les s√©ries temporelles car :\n",
    "- ‚úÖ Ils capturent les **d√©pendances √† long terme** (tendances)\n",
    "- ‚úÖ Ils g√®rent mieux les **variations temporelles** complexes\n",
    "- ‚úÖ Ils sont **robustes au probl√®me du gradient qui dispara√Æt**\n",
    "\n",
    "**Applications** :\n",
    "- Pr√©diction de prix d'actions (patterns sur plusieurs jours/semaines)\n",
    "- Pr√©visions m√©t√©orologiques (saisonnalit√© complexe)\n",
    "- D√©tection d'anomalies dans les capteurs IoT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Construction du mod√®le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(lookback):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le LSTM pour la pr√©diction de s√©ries temporelles.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Couche LSTM\n",
    "        layers.LSTM(64, input_shape=(lookback, 1), name='lstm'),\n",
    "        \n",
    "        # Dropout pour r√©gularisation\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.Dense(1, name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "lstm_model = create_lstm_model(LOOKBACK)\n",
    "\n",
    "# Affichage de l'architecture\n",
    "lstm_model.summary()\n",
    "\n",
    "# Comptage des param√®tres\n",
    "total_params_lstm = lstm_model.count_params()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Total de param√®tres LSTM: {total_params_lstm:,}\")\n",
    "print(f\"Ratio LSTM/RNN: {total_params_lstm/total_params_rnn:.2f}√ó\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Compilation et entra√Ænement du mod√®le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "lstm_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le LSTM compil√© avec succ√®s !\")\n",
    "\n",
    "# Callbacks\n",
    "live_plot_lstm = LivePlotCallback()\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement du mod√®le LSTM...\\n\")\n",
    "\n",
    "history_lstm = lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[live_plot_lstm, early_stopping_lstm],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© ! (Early stopping √† l'epoch {len(history_lstm.history['loss'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 √âvaluation du mod√®le LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_pred_lstm_scaled = lstm_model.predict(X_test, verbose=0)\n",
    "y_pred_lstm_original = scaler.inverse_transform(y_pred_lstm_scaled)\n",
    "\n",
    "# M√©triques\n",
    "mse_lstm = mean_squared_error(y_test_original, y_pred_lstm_original)\n",
    "mae_lstm = mean_absolute_error(y_test_original, y_pred_lstm_original)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_original, y_pred_lstm_original)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"R√âSULTATS FINAUX - MOD√àLE LSTM\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MSE (Mean Squared Error):  {mse_lstm:.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae_lstm:.2f} kWh\")\n",
    "print(f\"RMSE (Root MSE):           {rmse_lstm:.2f} kWh\")\n",
    "print(f\"R¬≤ Score:                  {r2_lstm:.4f}\")\n",
    "print(f\"\\nüìä Am√©lioration vs RNN: {((mae_rnn - mae_lstm)/mae_rnn * 100):.1f}% de r√©duction de la MAE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Visualisation des pr√©dictions LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Vue d'ensemble\n",
    "axes[0].plot(y_test_original, label='Valeurs R√©elles', linewidth=1.5, color='steelblue', alpha=0.8)\n",
    "axes[0].plot(y_pred_lstm_original, label='Pr√©dictions LSTM', linewidth=1.5, color='darkgreen', alpha=0.8)\n",
    "axes[0].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('LSTM - Pr√©dictions vs R√©alit√© (Set de Test Complet)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur une semaine\n",
    "zoom_size = 24 * 7\n",
    "axes[1].plot(range(zoom_size), y_test_original[:zoom_size], \n",
    "            label='Valeurs R√©elles', linewidth=2, color='steelblue', marker='o', markersize=4)\n",
    "axes[1].plot(range(zoom_size), y_pred_lstm_original[:zoom_size], \n",
    "            label='Pr√©dictions LSTM', linewidth=2, color='darkgreen', marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('LSTM - Zoom sur 1 Semaine de Pr√©dictions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des erreurs\n",
    "errors_lstm = y_test_original.flatten() - y_pred_lstm_original.flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax1.hist(errors_lstm, bins=50, color='darkgreen', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erreur = 0')\n",
    "ax1.axvline(x=errors_lstm.mean(), color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'Erreur moyenne: {errors_lstm.mean():.2f} kWh')\n",
    "ax1.set_xlabel('Erreur de Pr√©diction (kWh)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des Erreurs - LSTM', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.scatter(y_test_original, y_pred_lstm_original, alpha=0.5, s=20, color='darkgreen')\n",
    "ax2.plot([y_test_original.min(), y_test_original.max()], \n",
    "        [y_test_original.min(), y_test_original.max()], \n",
    "        'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "ax2.set_xlabel('Valeurs R√©elles (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Pr√©dictions (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Pr√©dictions vs R√©alit√© - LSTM (R¬≤={r2_lstm:.3f})', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# PARTIE 3 : GRU\n",
    "\n",
    "## 7. Architecture GRU pour S√©ries Temporelles\n",
    "\n",
    "### Avantages du GRU pour les s√©ries temporelles\n",
    "\n",
    "Le **GRU** est souvent le choix pr√©f√©r√© en production pour les s√©ries temporelles :\n",
    "- ‚úÖ **Performance proche du LSTM** avec moins de param√®tres\n",
    "- ‚úÖ **Entra√Ænement plus rapide** (important pour re-entra√Ænement fr√©quent)\n",
    "- ‚úÖ **Moins de risque d'overfitting** sur des datasets limit√©s\n",
    "- ‚úÖ **D√©ploiement plus l√©ger** (edge computing, IoT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Construction du mod√®le GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(lookback):\n",
    "    \"\"\"\n",
    "    Cr√©e un mod√®le GRU pour la pr√©diction de s√©ries temporelles.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Couche GRU\n",
    "        layers.GRU(64, input_shape=(lookback, 1), name='gru'),\n",
    "        \n",
    "        # Dropout\n",
    "        layers.Dropout(0.2, name='dropout'),\n",
    "        \n",
    "        # Couche de sortie\n",
    "        layers.Dense(1, name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Cr√©ation du mod√®le\n",
    "gru_model = create_gru_model(LOOKBACK)\n",
    "\n",
    "# Affichage de l'architecture\n",
    "gru_model.summary()\n",
    "\n",
    "# Comptage des param√®tres\n",
    "total_params_gru = gru_model.count_params()\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"Total de param√®tres GRU:  {total_params_gru:,}\")\n",
    "print(f\"Ratio GRU/RNN:  {total_params_gru/total_params_rnn:.2f}√ó\")\n",
    "print(f\"Ratio GRU/LSTM: {total_params_gru/total_params_lstm:.2f}√ó (GRU ~25% plus l√©ger)\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Compilation et entra√Ænement du mod√®le GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation\n",
    "gru_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Mod√®le GRU compil√© avec succ√®s !\")\n",
    "\n",
    "# Callbacks\n",
    "live_plot_gru = LivePlotCallback()\n",
    "early_stopping_gru = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Entra√Ænement\n",
    "print(\"\\nüöÄ D√©but de l'entra√Ænement du mod√®le GRU...\\n\")\n",
    "\n",
    "history_gru = gru_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[live_plot_gru, early_stopping_gru],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Entra√Ænement termin√© ! (Early stopping √† l'epoch {len(history_gru.history['loss'])})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 √âvaluation du mod√®le GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions\n",
    "y_pred_gru_scaled = gru_model.predict(X_test, verbose=0)\n",
    "y_pred_gru_original = scaler.inverse_transform(y_pred_gru_scaled)\n",
    "\n",
    "# M√©triques\n",
    "mse_gru = mean_squared_error(y_test_original, y_pred_gru_original)\n",
    "mae_gru = mean_absolute_error(y_test_original, y_pred_gru_original)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_original, y_pred_gru_original)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"R√âSULTATS FINAUX - MOD√àLE GRU\")\n",
    "print(\"=\"*70)\n",
    "print(f\"MSE (Mean Squared Error):  {mse_gru:.2f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae_gru:.2f} kWh\")\n",
    "print(f\"RMSE (Root MSE):           {rmse_gru:.2f} kWh\")\n",
    "print(f\"R¬≤ Score:                  {r2_gru:.4f}\")\n",
    "print(f\"\\nüìä Am√©lioration vs RNN:  {((mae_rnn - mae_gru)/mae_rnn * 100):.1f}% de r√©duction de la MAE\")\n",
    "print(f\"üìä Comparaison vs LSTM: {abs((mae_lstm - mae_gru)/mae_lstm * 100):.1f}% {'meilleur' if mae_gru < mae_lstm else 'moins bon'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Visualisation des pr√©dictions GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Vue d'ensemble\n",
    "axes[0].plot(y_test_original, label='Valeurs R√©elles', linewidth=1.5, color='steelblue', alpha=0.8)\n",
    "axes[0].plot(y_pred_gru_original, label='Pr√©dictions GRU', linewidth=1.5, color='purple', alpha=0.8)\n",
    "axes[0].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('GRU - Pr√©dictions vs R√©alit√© (Set de Test Complet)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur une semaine\n",
    "zoom_size = 24 * 7\n",
    "axes[1].plot(range(zoom_size), y_test_original[:zoom_size], \n",
    "            label='Valeurs R√©elles', linewidth=2, color='steelblue', marker='o', markersize=4)\n",
    "axes[1].plot(range(zoom_size), y_pred_gru_original[:zoom_size], \n",
    "            label='Pr√©dictions GRU', linewidth=2, color='purple', marker='s', markersize=4)\n",
    "axes[1].set_xlabel('Temps (heures)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('GRU - Zoom sur 1 Semaine de Pr√©dictions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution des erreurs\n",
    "errors_gru = y_test_original.flatten() - y_pred_gru_original.flatten()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax1.hist(errors_gru, bins=50, color='purple', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Erreur = 0')\n",
    "ax1.axvline(x=errors_gru.mean(), color='orange', linestyle='--', linewidth=2, \n",
    "           label=f'Erreur moyenne: {errors_gru.mean():.2f} kWh')\n",
    "ax1.set_xlabel('Erreur de Pr√©diction (kWh)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Fr√©quence', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution des Erreurs - GRU', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.scatter(y_test_original, y_pred_gru_original, alpha=0.5, s=20, color='purple')\n",
    "ax2.plot([y_test_original.min(), y_test_original.max()], \n",
    "        [y_test_original.min(), y_test_original.max()], \n",
    "        'r--', linewidth=2, label='Pr√©diction parfaite')\n",
    "ax2.set_xlabel('Valeurs R√©elles (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Pr√©dictions (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Pr√©dictions vs R√©alit√© - GRU (R¬≤={r2_gru:.3f})', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. COMPARAISON FINALE : RNN vs LSTM vs GRU\n",
    "\n",
    "### 8.1 Tableau comparatif des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tableau comparatif\n",
    "comparison_data = {\n",
    "    'M√©trique': ['MAE (kWh)', 'RMSE (kWh)', 'R¬≤ Score', 'Param√®tres', 'Epochs (Early Stop)'],\n",
    "    'RNN Simple': [\n",
    "        f\"{mae_rnn:.2f}\",\n",
    "        f\"{rmse_rnn:.2f}\",\n",
    "        f\"{r2_rnn:.4f}\",\n",
    "        f\"{total_params_rnn:,}\",\n",
    "        f\"{len(history_rnn.history['loss'])}\"\n",
    "    ],\n",
    "    'LSTM': [\n",
    "        f\"{mae_lstm:.2f}\",\n",
    "        f\"{rmse_lstm:.2f}\",\n",
    "        f\"{r2_lstm:.4f}\",\n",
    "        f\"{total_params_lstm:,}\",\n",
    "        f\"{len(history_lstm.history['loss'])}\"\n",
    "    ],\n",
    "    'GRU': [\n",
    "        f\"{mae_gru:.2f}\",\n",
    "        f\"{rmse_gru:.2f}\",\n",
    "        f\"{r2_gru:.4f}\",\n",
    "        f\"{total_params_gru:,}\",\n",
    "        f\"{len(history_gru.history['loss'])}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"COMPARAISON FINALE : RNN vs LSTM vs GRU - S√©ries Temporelles\")\n",
    "print(\"=\"*100)\n",
    "print(df_comparison.to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# D√©termination du meilleur mod√®le\n",
    "maes = [mae_rnn, mae_lstm, mae_gru]\n",
    "model_names = ['RNN', 'LSTM', 'GRU']\n",
    "best_idx = np.argmin(maes)\n",
    "print(f\"\\nüèÜ Meilleur mod√®le en MAE : {model_names[best_idx]} ({maes[best_idx]:.2f} kWh)\")\n",
    "\n",
    "r2s = [r2_rnn, r2_lstm, r2_gru]\n",
    "best_r2_idx = np.argmax(r2s)\n",
    "print(f\"üèÜ Meilleur mod√®le en R¬≤ : {model_names[best_r2_idx]} ({r2s[best_r2_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Visualisation comparative des pr√©dictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle sur une semaine\n",
    "zoom_size = 24 * 7  # 1 semaine\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "fig.suptitle('Comparaison RNN vs LSTM vs GRU - S√©ries Temporelles', \n",
    "            fontsize=18, fontweight='bold', y=0.995)\n",
    "\n",
    "# Vue compl√®te\n",
    "axes[0, 0].plot(y_test_original, label='R√©alit√©', linewidth=1.5, color='black', alpha=0.6)\n",
    "axes[0, 0].plot(y_pred_rnn_original, label='RNN', linewidth=1, color='#3498db', alpha=0.8)\n",
    "axes[0, 0].plot(y_pred_lstm_original, label='LSTM', linewidth=1, color='#2ecc71', alpha=0.8)\n",
    "axes[0, 0].plot(y_pred_gru_original, label='GRU', linewidth=1, color='#9b59b6', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Temps (heures)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Consommation (kWh)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Comparaison sur le Test Set Complet', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Zoom sur 1 semaine\n",
    "axes[0, 1].plot(range(zoom_size), y_test_original[:zoom_size], \n",
    "               label='R√©alit√©', linewidth=2.5, color='black', marker='o', markersize=3)\n",
    "axes[0, 1].plot(range(zoom_size), y_pred_rnn_original[:zoom_size], \n",
    "               label='RNN', linewidth=1.5, color='#3498db', marker='s', markersize=2)\n",
    "axes[0, 1].plot(range(zoom_size), y_pred_lstm_original[:zoom_size], \n",
    "               label='LSTM', linewidth=1.5, color='#2ecc71', marker='^', markersize=2)\n",
    "axes[0, 1].plot(range(zoom_size), y_pred_gru_original[:zoom_size], \n",
    "               label='GRU', linewidth=1.5, color='#9b59b6', marker='d', markersize=2)\n",
    "axes[0, 1].set_xlabel('Temps (heures)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Consommation (kWh)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Zoom sur 1 Semaine', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Comparaison des MAE\n",
    "models = ['RNN', 'LSTM', 'GRU']\n",
    "maes_list = [mae_rnn, mae_lstm, mae_gru]\n",
    "colors = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "bars = axes[1, 0].bar(models, maes_list, color=colors, edgecolor='black', linewidth=2, width=0.6)\n",
    "axes[1, 0].set_ylabel('MAE (kWh)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Comparaison de la MAE', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "for bar, mae in zip(bars, maes_list):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{mae:.2f} kWh',\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Comparaison des R¬≤\n",
    "r2s_list = [r2_rnn, r2_lstm, r2_gru]\n",
    "bars2 = axes[1, 1].bar(models, r2s_list, color=colors, edgecolor='black', linewidth=2, width=0.6)\n",
    "axes[1, 1].set_ylabel('R¬≤ Score', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Comparaison du R¬≤ Score', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_ylim([min(r2s_list) - 0.02, 1.0])\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "for bar, r2 in zip(bars2, r2s_list):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{r2:.4f}',\n",
    "                   ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Comparaison des courbes d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "fig.suptitle('Courbes d\\'Apprentissage - Comparaison', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors_comp = ['#3498db', '#2ecc71', '#9b59b6']\n",
    "\n",
    "# Loss de validation\n",
    "axes[0].plot(history_rnn.history['val_loss'], 'o-', label='RNN', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[0])\n",
    "axes[0].plot(history_lstm.history['val_loss'], 's-', label='LSTM', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[1])\n",
    "axes[0].plot(history_gru.history['val_loss'], '^-', label='GRU', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[2])\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Validation Loss (MSE)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('√âvolution de la Loss de Validation', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# MAE de validation\n",
    "axes[1].plot(history_rnn.history['val_mae'], 'o-', label='RNN', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[0])\n",
    "axes[1].plot(history_lstm.history['val_mae'], 's-', label='LSTM', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[1])\n",
    "axes[1].plot(history_gru.history['val_mae'], '^-', label='GRU', \n",
    "            linewidth=2.5, markersize=6, color=colors_comp[2])\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Validation MAE', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('√âvolution de la MAE de Validation', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pr√©diction Multi-Step (Bonus)\n",
    "\n",
    "### Pr√©dire plusieurs heures √† l'avance\n",
    "\n",
    "Dans la pratique, on veut souvent pr√©dire **plusieurs timesteps futurs** :\n",
    "- Pr√©dire les 24 prochaines heures\n",
    "- Pr√©dire la semaine suivante\n",
    "\n",
    "**M√©thodes** :\n",
    "1. **Recursive** : Pr√©dire t+1, puis utiliser cette pr√©diction pour pr√©dire t+2, etc.\n",
    "2. **Direct** : Entra√Æner un mod√®le s√©par√© pour chaque horizon\n",
    "3. **Multi-output** : Un seul mod√®le qui pr√©dit tous les horizons simultan√©ment\n",
    "\n",
    "Testons l'approche **recursive** avec le meilleur mod√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_multi_step(model, initial_sequence, n_steps, scaler):\n",
    "    \"\"\"\n",
    "    Pr√©dit n_steps √† l'avance de mani√®re r√©cursive.\n",
    "    \n",
    "    Args:\n",
    "        model: Mod√®le entra√Æn√©\n",
    "        initial_sequence: S√©quence initiale normalis√©e (lookback, 1)\n",
    "        n_steps: Nombre d'√©tapes √† pr√©dire\n",
    "        scaler: Scaler pour d√©normalisation\n",
    "    \n",
    "    Returns:\n",
    "        Pr√©dictions d√©normalis√©es\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    current_sequence = initial_sequence.copy()\n",
    "    \n",
    "    for _ in range(n_steps):\n",
    "        # Pr√©dire le prochain timestep\n",
    "        current_input = current_sequence.reshape(1, LOOKBACK, 1)\n",
    "        next_pred = model.predict(current_input, verbose=0)[0, 0]\n",
    "        predictions.append(next_pred)\n",
    "        \n",
    "        # Mettre √† jour la s√©quence (glisser la fen√™tre)\n",
    "        current_sequence = np.append(current_sequence[1:], next_pred)\n",
    "    \n",
    "    # D√©normaliser\n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    predictions_original = scaler.inverse_transform(predictions)\n",
    "    \n",
    "    return predictions_original.flatten()\n",
    "\n",
    "# Choisir le meilleur mod√®le\n",
    "if mae_gru <= mae_lstm and mae_gru <= mae_rnn:\n",
    "    best_model = gru_model\n",
    "    best_name = 'GRU'\n",
    "elif mae_lstm <= mae_rnn:\n",
    "    best_model = lstm_model\n",
    "    best_name = 'LSTM'\n",
    "else:\n",
    "    best_model = rnn_model\n",
    "    best_name = 'RNN'\n",
    "\n",
    "print(f\"Utilisation du mod√®le {best_name} pour la pr√©diction multi-step\\n\")\n",
    "\n",
    "# Pr√©dire les 72 prochaines heures (3 jours)\n",
    "n_future_steps = 72\n",
    "start_idx = 0\n",
    "\n",
    "initial_seq = X_test[start_idx].flatten()\n",
    "predictions_future = predict_multi_step(best_model, initial_seq, n_future_steps, scaler)\n",
    "\n",
    "# Valeurs r√©elles correspondantes\n",
    "actual_future = y_test_original[start_idx:start_idx+n_future_steps].flatten()\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10))\n",
    "\n",
    "# Pr√©diction 72h √† l'avance\n",
    "time_range = range(n_future_steps)\n",
    "ax1.plot(time_range, actual_future, label='Valeurs R√©elles', \n",
    "        linewidth=2.5, color='steelblue', marker='o', markersize=4)\n",
    "ax1.plot(time_range, predictions_future, label=f'Pr√©dictions {best_name} (Recursive)', \n",
    "        linewidth=2.5, color='crimson', marker='s', markersize=4)\n",
    "ax1.set_xlabel('Heures √† l\\'avance', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Consommation (kWh)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Pr√©diction Multi-Step - {n_future_steps}h √† l\\'avance ({best_name})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Erreur cumulative\n",
    "errors_cumulative = np.abs(actual_future - predictions_future)\n",
    "ax2.plot(time_range, errors_cumulative, linewidth=2.5, color='darkgreen', marker='o', markersize=4)\n",
    "ax2.axhline(y=errors_cumulative.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Erreur moyenne: {errors_cumulative.mean():.2f} kWh')\n",
    "ax2.set_xlabel('Heures √† l\\'avance', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Erreur Absolue (kWh)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('√âvolution de l\\'Erreur de Pr√©diction', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mae_multistep = mean_absolute_error(actual_future, predictions_future)\n",
    "print(f\"\\nüìä MAE pour la pr√©diction {n_future_steps}h √† l'avance: {mae_multistep:.2f} kWh\")\n",
    "print(f\"üìä D√©gradation vs pr√©diction 1h: {((mae_multistep - eval(f'mae_{best_name.lower()}')) / eval(f'mae_{best_name.lower()}') * 100):.1f}%\")\n",
    "print(f\"\\n‚ö†Ô∏è  Note: L'erreur augmente avec l'horizon de pr√©diction (effet cumulatif)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion et Points Cl√©s\n",
    "\n",
    "### üìö Ce que nous avons appris\n",
    "\n",
    "#### 1. **Pr√©paration des Donn√©es pour S√©ries Temporelles**\n",
    "- ‚úÖ Fen√™tre glissante (sliding window) pour cr√©er les s√©quences\n",
    "- ‚úÖ Normalisation essentielle pour les RNN\n",
    "- ‚úÖ Split temporel (pas de shuffle !)\n",
    "- ‚úÖ Lookback = compromis entre contexte et complexit√©\n",
    "\n",
    "#### 2. **RNN Simple**\n",
    "- ‚úÖ Architecture de base, rapide √† entra√Æner\n",
    "- ‚úÖ Fonctionne bien sur s√©quences courtes avec patterns simples\n",
    "- ‚ùå Difficult√© avec d√©pendances √† long terme\n",
    "- ‚ùå Gradient qui dispara√Æt sur longues s√©quences\n",
    "\n",
    "#### 3. **LSTM**\n",
    "- ‚úÖ Excellente capture des d√©pendances temporelles longues\n",
    "- ‚úÖ G√®re les tendances et saisonnalit√©s complexes\n",
    "- ‚úÖ Architecture de r√©f√©rence pour s√©ries temporelles\n",
    "- ‚ùå Plus de param√®tres = risque d'overfitting\n",
    "- ‚ùå Entra√Ænement plus lent\n",
    "\n",
    "#### 4. **GRU**\n",
    "- ‚úÖ Performance comparable au LSTM\n",
    "- ‚úÖ Moins de param√®tres (~25% de moins)\n",
    "- ‚úÖ Plus rapide √† entra√Æner et √† d√©ployer\n",
    "- ‚úÖ **Choix recommand√© pour la production**\n",
    "\n",
    "### üéØ Recommandations Pratiques\n",
    "\n",
    "| Situation | Recommandation |\n",
    "|-----------|----------------|\n",
    "| **Prototypage rapide** | GRU (meilleur rapport perf/vitesse) |\n",
    "| **Donn√©es limit√©es** | RNN ou GRU (moins de risque d'overfitting) |\n",
    "| **Patterns complexes** | LSTM (meilleure capacit√© d'apprentissage) |\n",
    "| **Production/Edge** | GRU (l√©ger et rapide) |\n",
    "| **Pr√©diction temps r√©el** | GRU (inf√©rence rapide) |\n",
    "| **Lookback court (<24h)** | RNN peut suffire |\n",
    "| **Lookback long (>1 semaine)** | LSTM ou GRU obligatoire |\n",
    "\n",
    "### üîß Optimisations Possibles\n",
    "\n",
    "1. **Architecture** :\n",
    "   - Bidirectional RNN/LSTM/GRU\n",
    "   - Stacked layers (2-3 couches r√©currentes)\n",
    "   - Residual connections\n",
    "   - Attention mechanisms\n",
    "\n",
    "2. **Features** :\n",
    "   - Ajout de features temporelles (heure, jour, mois)\n",
    "   - Features externes (temp√©rature, jours f√©ri√©s)\n",
    "   - Lags multiples\n",
    "\n",
    "3. **Entra√Ænement** :\n",
    "   - Learning rate scheduling\n",
    "   - Augmentation de donn√©es temporelles\n",
    "   - Cross-validation temporelle (Time Series Split)\n",
    "\n",
    "### üöÄ Pour Aller Plus Loin\n",
    "\n",
    "**Architectures modernes** :\n",
    "- **Temporal Convolutional Networks (TCN)** : CNN 1D pour s√©ries temporelles\n",
    "- **Transformers** : Attention-based models (Temporal Fusion Transformer)\n",
    "- **N-BEATS** : Architecture sp√©cialis√©e pour forecasting\n",
    "- **Prophet / NeuralProphet** : Mod√®les de Meta pour s√©ries temporelles\n",
    "\n",
    "**Techniques avanc√©es** :\n",
    "- Pr√©diction probabiliste (quantiles, intervalles de confiance)\n",
    "- D√©tection d'anomalies temporelles\n",
    "- Multi-variate forecasting\n",
    "- Transfer learning sur s√©ries temporelles\n",
    "\n",
    "### üí° Message Final\n",
    "\n",
    "**Les RNN/LSTM/GRU sont des outils puissants pour les s√©ries temporelles** :\n",
    "- Ils capturent naturellement les d√©pendances temporelles\n",
    "- Le **GRU** offre le meilleur compromis en production\n",
    "- Les **LSTM** restent la r√©f√©rence pour les patterns complexes\n",
    "- La **pr√©paration des donn√©es** est cruciale (normalisation, lookback)\n",
    "\n",
    "**Note importante** : Les Transformers commencent √† dominer m√™me les s√©ries temporelles, mais les RNN/LSTM/GRU restent pertinents car :\n",
    "- ‚úÖ Plus l√©gers et rapides\n",
    "- ‚úÖ N√©cessitent moins de donn√©es\n",
    "- ‚úÖ Excellents pour l'edge computing / IoT\n",
    "- ‚úÖ Interpr√©tabilit√© sup√©rieure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Exercices Pratiques\n",
    "\n",
    "### Exercice 1 : Modifier le Lookback\n",
    "- Testez avec un lookback de 24h (1 jour) vs 336h (2 semaines)\n",
    "- Comparez les performances et le temps d'entra√Ænement\n",
    "\n",
    "### Exercice 2 : Architecture Bidirectionnelle\n",
    "- Modifiez le GRU pour utiliser `Bidirectional(GRU(64))`\n",
    "- Est-ce que cela am√©liore les performances sur ce probl√®me ?\n",
    "\n",
    "### Exercice 3 : Features Suppl√©mentaires\n",
    "- Ajoutez l'heure du jour et le jour de la semaine comme features\n",
    "- Modifiez l'architecture pour accepter 3 features au lieu de 1\n",
    "\n",
    "### Exercice 4 : Stacked RNN\n",
    "- Cr√©ez un mod√®le avec 2 couches GRU empil√©es\n",
    "- Utilisez `return_sequences=True` sur la premi√®re couche\n",
    "\n",
    "### Exercice 5 : Autre S√©rie Temporelle\n",
    "- Appliquez ces techniques sur vos propres donn√©es\n",
    "- Prix d'actions, temp√©rature, trafic r√©seau, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
